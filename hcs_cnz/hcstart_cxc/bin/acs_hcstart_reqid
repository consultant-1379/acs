#*********************************************************
# --------------------------------------------------------
#     Copyright (C) 2012 Ericsson AB. All rights reserved.
# --------------------------------------------------------
##
# Name:
#       acs_hcstart_reqid
# Description:
#       A script containing the functionality of HC requirements
#
##
# Usage: 
#	It is an internal script which is called from hcstart command
#
##
# Output:
#       Returns the success or failure of requirements to parent script
##
#
#
# Changelog:
# - Wed Aug 22 2023 Koti Kiran Maddi (ZKTMOAD)
#   Provided fix for TR IA33117
# - Mon Jul 10 2023 Koti Kiran Maddi (ZKTMOAD)
#   Provided fix for TR IA40471 
# - Tue Dec 27 2022 Srimukhi Nagalla (ZSGRAIN)
#   Provided fix for Issue 274 related to NTP synchronization fault
# - Fri Jan 21 2022 Sowjanya Medak (XSOWMED)
#   Removed telnet sockets
# - Thu Aug 26 2021 T Sravanthi (XSRAVAN)
#   Added fix for TR(HZ35848) for Rule 39
# - Tue Jul 27 2021 Ramya Medichelmi (ZMEDRAM)
#   Added new rule IO_HCRS_56
# - Wed Jul 07 2021 Ramya Medichelmi (ZMEDRAM)
#   Rebase for TR HY95333
# - Tue Mar 30 2020 Pravalika P (ZPRAPXX)
#   TR HY94387 fix and LDAP User Authorization alarm fix
# - Tue Jan 05 2020 Pravalika P (ZPRAPXX),Rajeshwari P(XCSRPAD)
#   Improvement in rule 52 and 54
# - Mon Dec 28 2020 Gnaneswara Seshu(ZBHEGNA)
#   Added new rules IO_HCRS_55
# - Tue Dec 15 2020 Pravalika P(ZPRAPXX), Suryanarayana Pammi(XPAMSUR)
#   Modified rules IO_HCRS_51,IO_HCRS_53
# - Fri Dec 11 2020 Suryanarayana Pammi (XPAMSUR), Rajeshwari P(XCSRPAD), Pravalika P(ZPRAPXX)
#   Added new rules for UFR PH2:IO_HCRS_50,IO_HCRS_51,IO_HCRS_52,IO_HCRS_53,IO_HCRS_54
# - Tue Dec 01 202 Swapnika Baradi (xswapba)
#   Fix for TR HY66615
# - Fri Jul 31 2020 Ganoz Siva kumar (XSIGANO)
#   Added IO_HCRS_46, IO_HCRS_47, IO_HCRS_48 and IO_HCRS_49 for HCSTART improvement
# - Thu Jul 30 2020 Swapnika B (xswapba)
#   Added IO_HCRS_45 to check NTP external server connectivity on vAPZ
# - Thr Jul 30 2020 AnjiReddy Daka (xdakanj)
#   Added IO_HCRS_42, IO_HCRS_43 and IO_HCRS_44 for HCSTART improvement
# - Mon Jan 07 2019 Naveen G (zgxxnav)
#   Modified IO_HCRS_37 to fix HX42070
# - Tue Jan 09 2018 Swetha rambathini (xsweram)
#   Modified IO_HCRS_37 to support deprecation of SwVersionMain MO
# - Wed Jul 26 2017 Sowjanya M (xsowmed)
#   Modified IO_HCRS_34 for gep 7
# - Tue Nov 7 2017 Furquan Ullah (XFURULL)
#  Modified IO_HCRS_17 for SLES 12 SP 2 
# - Tue Jun 7 2017 Yeswanth Vankayala (xyesvan)
#  Modified IO_HCRS_32 for TR fix HV85579 
# - Fri May 19 2017 Furquan Ullah (xfurull)
#	Implemented the fix for TR HV83480 in IO_HCRS_31
# - Mon Feb 13 2017 Avinash Gundlapally (xavigun)
#	Implemented the changes to support ssh subsystem in APG
# - Tue Feb 23 2016 Alessio Cascone (EALOCAE)
# 	Implemented changes for SLES12 adaptation.
#- Tue Apr 12 2016 Furquan Ullah (XFURULL)
#       Modified IO_HCSRS_3 for TR HU25530
#- Fri Sep 25 2015 Kiran Apuri (XKIRAPU)
#       Modified IO_HCSRS_39 for TR HU18995
# - Wed APr 15 2015 Divya Chakkilam (XCHADIV)
#	CNI Rebase 109 22-APZ 212 33/6-460 for TR HT38928 and TR HT46695
# - Mon May 19 2014 - Shyam Chirania (XSHYCHI)
#       Modifying IO_HCRS_37,IO_HCRS_38 and IO_HCRS_32 for HS48814 during CNI349 rebase
# - Mon May 19 2014 - Shyam Chirania (XSHYCHI)
#       Modifying IO_HCRS_13 for HS23210 and  IO_HCRS_31 for HS40807 during CNI349 rebase
# - Mar 05 2014 - Trotta A. (xanttro)	
#	HCSTART improvement: 
#   integration with the AP, System Restore, Initiate step from 48 to 75
# - Fri Oct 04 2013 - Greeshmalatha C (XGRECHA)
#	Modifying IO_HCRS_19 for HR80093
# - Thu Sep 12 2013 - Greeshmalatha C (XGRECHA)
#	Modifying IO_HCRS_37 and adding IO_HCRS_38 and IO_HCRS_14
# - Thu Aug 22 2013 -Greeshmalatha C (XGRECHA)
#	Excluding IO_HCRS_14,IO_HCRS_17,IO_HCRS_20,IO_HCRS_22,IO_HCRS_28,IO_HCRS_36
#	from option "-l"
# - Mon Mat 13 2013 -Greeshmalatha C (XGRECHA)
#	Added check for SW installed (IO_HCRS_36) and sanity for SWU (IO_HCRS_37)
#
# - Wed Apr 16 2013 -Greeshmalatha C(XGRECHA)
#	Added a check for the length and Case of ME-ID(APG name) in IO_HCRS_35
#
# - Sun May 27 2012 - Satya Deepthi (xsatdee)
#       First version.
#*********************************************************

. /opt/ap/acs/bin/acs_hcstart_common

# Common Files
peer_node_file="/etc/cluster/nodes/peer/hostname"
internal_ip_file="/etc/cluster/nodes/this/networks/internal/primary/address"
pswd_file="/cluster/etc/passwd"
ssu_config_file="/opt/ap/acs/etc/SSU_Quotas"
cliss_subshell_file="/cluster/storage/system/config/com-apr9010443/lib/comp/libcli_extension_subshell.cfg"
ssh_config="/etc/ssh/ssh_config"
sshd_config_22="/etc/ssh/sshd_config_22"
sshd_config_830="/etc/ssh/sshd_config_830"
sshd_internal="/etc/ssh/sshd_config_internal"
vlan_config="/proc/net/vlan/config"
HWTYPE="/opt/ap/apos/conf/apos_hwinfo.sh"
is_swm_2_0="/cluster/storage/system/config/apos/swm_version"
CUT="/usr/bin/cut"

launchCommand=
output=
remote_node=
tmp_file=
nodeAName=
nodeBName=
result=
singleCP=0
multiCP=0

# Commands
CMW_STATUS="/opt/coremw/bin/cmw-status"
CMW_REPOSITORY="/opt/coremw/bin/cmw-repository-list"
SMARTCTL="/usr/sbin/smartctl"
PRCSTATE="/usr/bin/prcstate"
FTPLS="/usr/bin/ftpls"
VDLS="/usr/bin/vdls"
DMIDECODE="/usr/sbin/dmidecode"
HOSTNAME="/bin/hostname"
ALIST="/usr/bin/alist"
LS="/bin/ls"
LISTTSUSER="/usr/bin/listtsuser"
DATE="/bin/date"
SERVICE="/sbin/service"
SERVICEMGMT="/opt/ap/apos/bin/servicemgmt/servicemgmt"
HWMSCBLS="/usr/bin/hwmscbls"
HWMXLS="/usr/bin/hwmxls"
RAIDMGMT="/usr/bin/raidmgmt"
RAIDMGR="/usr/bin/raidmgr"
MML="/usr/bin/mml"
INTEGRITYAP="/usr/bin/integrityap"
PS="/bin/ps"
VLANLS="/usr/bin/vlanls"
CHAGE="/usr/bin/chage"
REPQUOTA="/usr/sbin/repquota"
FDISK="/sbin/fdisk"
LDAPSEARCH="/usr/bin/ldapsearch"
IPMITOOL="/usr/sbin/eri-ipmitool"
CLUSTER="/usr/bin/cluster"
CHKCONFIG="/sbin/chkconfig"
IMMFIND="/usr/bin/immfind"
IMMLIST="/usr/bin/immlist"
LSSCSI="/usr/bin/lsscsi"
DIFF="/usr/bin/diff"
WC="/usr/bin/wc"
APGADM="/usr/bin/apg-adm"
CUT="/usr/bin/cut"
DRBDADM="/sbin/drbdadm"
FIND="/usr/bin/find"

# Common functions
function check_peer_node_state(){

	remote_node=`$CAT $peer_node_file`
	$PING -c 2 $remote_node &> /dev/null
	if [ $? == 0 ]; then
		return $exit_success
	else
		return $exit_failure
	fi
}

function check_gep_info(){
        #local gep_info=$($IPMITOOL gp | $GREP ROJ | $AWK  '{print $5}' | $AWK '{print substr($0,5,1)}')
        # Added HWTYPE to implement HW decoupling
        local gep_info=$($HWTYPE --hwversion | awk  -F= '{print $2}' | awk '{print substr($0,4,1)}')
        local gep_type=${gep_info:0:1}
	echo $gep_type
	# If gep_type=1 -> GEP1
	# If gep_type=2 -> GEP2
}
function check_disk_info(){
	#local disk_type=$($IPMITOOL gp | $GREP ROJ | $AWK  '{print $5}' | $AWK '{print substr($0,0,3)}')
        # Added HWTYPE to implement HW decoupling
        local disk_type=$($HWTYPE --disksnumber | awk -F= '{print $2}' | awk '{print substr($0,0,1)}')
        echo $disk_type
        #  disk_type=1 -> GEP4/GEP5 -400
        #  disk_type=3 -> GEP4/GEP5 -1200
}


function check_shelf_info(){
	local axeFunctions_class=$($IMMFIND | $GREP -i axeFunctions | $HEAD -1)
	local shelf_info=$($IMMLIST -a apgShelfArchitecture $axeFunctions_class | $CUT -d= -f2)
	echo $shelf_info
	# If shelf_info=0 -> SCB(EGEM) architecture
	# If shelf_info=1 -> SCX(EGEM2) architecture
        # If shelf_info=2 -> BSP architecture
        # If shelf_info=4 -> SMX(EGEM2) architecture
}

function checkHealth() {

	nodeType=$1
	funcname=$2
	if [[ ! -z $result ]]; then
		#echo result, output: $result, $output.
                if [[ ! "$result" == "$output" ]]; then
                        #flog "Failed"
                        return $exit_failure
                fi
	else
		return $exit_failure
        fi
	return $exit_success
}

function check_cp_state() {

	cp_type=`immlist -a systemType axeFunctionsId=1 | $CUT -d= -f2`
	if [ $? != 0 ]; then
		flog "Failed to retrieve CP information"
		return $exit_failure
	fi
	if [ $cp_type == 0 ]; then
		singleCP=1
	else
		multiCP=1
	fi
}

function check_application_info(){
        local axeFunctions_class=$($IMMFIND | $GREP -i axeFunctions | $HEAD -1)
        local application_info=$($IMMLIST -a axeApplication $axeFunctions_class | $CUT -d= -f2)
        echo $application_info
        # If application info=0 -> MSC-S Application
        # If application info=1 -> HLR Application
        # If application info=2 -> BSC Application
        # If application info=3 -> WLN Application
        # If application info=4 -> TSC Application
        # If application info=5 -> IPSTP Application
}

function check_aptcmm_UpgradeStatus(){
        local EnmAPTCMMInstallationParameter_class=$($IMMFIND | $GREP -i EnmAPTCMMInstallationParameterId | $HEAD -1)
        
        local aptcmm_upgradestatus=$($IMMLIST -a UpgradeStatus $EnmAPTCMMInstallationParameter_class | $CUT -d= -f2)
        echo $aptcmm_upgradestatus
        # If application info=DONE -> No APTCMM upgrade ongoing

}

function check_aptcmm_InstallationStatus(){
        local EnmAPTCMMInstallationParameter_class=$($IMMFIND | $GREP -i EnmAPTCMMInstallationParameterId | $HEAD -1)
        local aptcmm_installationstatus=$($IMMLIST -a InstallationStatus $EnmAPTCMMInstallationParameter_class | $CUT -d= -f2)
        echo $aptcmm_installationstatus
        # If application info=DONE -> No APTCMM upgrade ongoing

}



#---------------------------------------------------------
function IO_HCRS_1(){

	trace_enter $FUNCNAME

	launchCommand1="$CAT /opt/ap/apos/conf/vsftpd/vsftpd-APIO_1.conf"
	launchCommand2="$CAT /opt/ap/apos/conf/vsftpd/vsftpd-APIO_2.conf"
	output="/var/cpftp"
	tmp_file=`$MKTEMP -t tempfile.XXXXXX`

	parsePrintout="$CAT $tmp_file | $AWK -F= '{if(\$1==\"anon_root\") print \$2}'"

	# Checking health on local node
	$launchCommand1 2>/dev/null > $tmp_file
	result=`eval $parsePrintout`
	checkHealth "local" $FUNCNAME
	if [ $? == $exit_failure ]; then
		flog "Check for root directory failed for APIO_1 on local node"
        	rm $tmp_file
		return $exit_failure
	fi
	flog "Check for root directory succeeded for APIO_1 on local node"

	$launchCommand2 2>/dev/null > $tmp_file
	result=`eval $parsePrintout`
	checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
		flog "Check for root directory failed for APIO_2 on local node"
        	rm $tmp_file
                return $exit_failure
        fi
	flog "Check for root directory succeeded for APIO_2 on local node"

	flog "Healthcheck succeeded on local node"

	# Check if -l option is specified
	if [ $a_local == $FALSE ]; then # If no -l option is specified, then check health of remote node
		# Checking status of peer node
		check_peer_node_state
		if [ $? == 0 ]; then

			# Launch command on remote node	
        		${SSH} -n $remote_node $launchCommand1 2>/dev/null > $tmp_file 
		        result=`eval $parsePrintout`
			checkHealth "remote" $FUNCNAME
		        if [ $? == $exit_failure ]; then
				flog "Check for root directory failed for APIO_1 on remote node"
        			rm $tmp_file
                		return $exit_failure
		        fi
			flog "Check for root directory succeeded for APIO_1 on remote node"

        		${SSH} -n $remote_node $launchCommand2 2>/dev/null > $tmp_file 
        		result=`eval $parsePrintout`
                        checkHealth "remote" $FUNCNAME
                        if [ $? == $exit_failure ]; then
				flog "Check for root directory failed for APIO_2 on remote node"
        			rm $tmp_file
                                return $exit_failure
                        fi
			flog "Check for root directory succeeded for APIO_1 on remote node"
			flog "Healthcheck succeeded on remote node"
		else 
			flog "Remote node is down. Health not checked on remote node"
		fi
	fi
		
	rm $tmp_file

	trace_leave $FUNCNAME
	return $exit_success
}

#---------------------------------------------------------
function IO_HCRS_2(){

	trace_enter $FUNCNAME

        launchCommand="$CAT /opt/ap/apos/conf/vsftpd/vsftpd.conf"
        output="/data/opt/ap/nbi_fuse"
        tmp_file=`mktemp -t tempfile.XXXXXX`

        parsePrintout="$CAT $tmp_file | $AWK -F= '{if(\$1==\"local_root\") print \$2}'"

        # Checking health on local node
        $launchCommand 2>/dev/null  > $tmp_file
        result=`eval $parsePrintout`
        checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
		flog "Check for root directory of Default FTP Site failed on local node"
        	rm $tmp_file
                return $exit_failure
        fi
	flog "Check for root directory of Default FTP Site succeeded on local node"

        flog "Healthcheck succeeded on local node"

        # Check if -l option is specified
        if [ $a_local == $FALSE ]; then # If no -l option is specified, then check health of remote node

                # Checking status of peer node
                check_peer_node_state
                if [ $? == 0 ]; then
                        # Launch command on remote node
                        ${SSH} -n $remote_node $launchCommand 2>/dev/null > $tmp_file
                        result=`eval $parsePrintout`
                        checkHealth "remote" $FUNCNAME
                        if [ $? == $exit_failure ]; then
				flog "Check for root directory of Default FTP Site failed on remote node"
        			rm $tmp_file
                                return $exit_failure
                        fi
			flog "Check for root directory of Default FTP Site succeeded on remote node"
        		flog "Healthcheck succeeded on remote node"
                else
                        flog "Remote node is down. Health not checked on remote node"
                fi
        fi

        rm $tmp_file

	trace_leave $FUNCNAME
	return $exit_success
}

#---------------------------------------------------------
function IO_HCRS_3(){

	trace_enter $FUNCNAME

        launchCommand1="$CAT /opt/ap/apos/conf/vdir/vd-conf" 

        output="APZ:/data/apz cpa:/data/apz/data/cpa/cphw/crash cpb:/data/apz/data/cpb/cphw/crash CPSDUMP:/data/cps/data CPSLOAD:/data/fms/data tracelog:/data/cps/logs"
        
	tmp_file=`mktemp -t tempfile.XXXXXX`
        parsePrintout="$CAT $tmp_file | $AWK '{OFS=\":\"; print \$2,\$3}' | $SORT -f"

        # Checking health on local node
        $launchCommand1 > $tmp_file 
        res=`eval $parsePrintout`
        result=`$ECHO $res`
	checkHealth "local" $FUNCNAME
	if [ $? == $exit_failure ]; then
		flog "Private Network FTP Server virtual directory wrongly defined on local node"
        	rm $tmp_file
                return $exit_failure
	fi
	flog "Private Network FTP Server virtual directory correctly defined on local node"
        flog "Healthcheck succeeded on local node"

        # Check if -l option is specified
        if [ $a_local == $FALSE ]; then # If no -l option is specified, then check health of remote node

                # Checking status of peer node
                check_peer_node_state
                if [ $? == 0 ]; then
                        # Launch command on remote node
                        ${SSH} -n $remote_node $launchCommand1 2>/dev/null > $tmp_file
		        res=`eval $parsePrintout`
        		result=`$ECHO $res`
                        checkHealth "remote" $FUNCNAME
                        if [ $? == $exit_failure ]; then
				flog "Private Network FTP Server virtual directory incorrectly defined on remote node"
        			rm $tmp_file
                                return $exit_failure
                        fi
			flog "Private Network FTP Server virtual directory correctly defined on remote node"
                        flog "Healthcheck succeeded on remote node"
                else
                        flog "Remote node is not up. Health not checked on remote node"
                fi
        fi

        rm $tmp_file

	trace_leave $FUNCNAME
	return $exit_success
}

#---------------------------------------------------------
function IO_HCRS_4(){

	trace_enter $FUNCNAME

        tmp_file=`mktemp -t tempfile.XXXXXX`
        launchCommand1="$CAT /opt/ap/apos/conf/vdir/vd-conf"

        parsePrintout="$CAT $tmp_file | $AWK '{OFS=\":\"; print \$2,\$3}' | $SORT -f"

        # Checking health on local node
        $launchCommand1 > $tmp_file
        res=`eval $parsePrintout`
        result=`$ECHO $res`

	# Checking status of peer node
        check_peer_node_state
        if [ $? == 0 ]; then
        	# Launch command on remote node
                ${SSH} -n $remote_node $launchCommand1 2>/dev/null > $tmp_file
                res=`eval $parsePrintout`
                output=`$ECHO $res`
                checkHealth "remote" $FUNCNAME
                if [ $? == $exit_failure ]; then
                	flog "Private Network FTP Server virtual directory incorrectly defined on remote node"
        		rm $tmp_file
                        return $exit_failure
		fi
                flog "Private Network FTP Server virtual directory correctly defined on remote node"
                flog "Healthcheck succeeded on remote node"
	else
        	flog "Remote node is not up. Health not checked"
        	rm $tmp_file
		return $exit_failure
        fi

        rm $tmp_file

	trace_leave $FUNCNAME
	return $exit_success
}

#---------------------------------------------------------
function IO_HCRS_5(){

	trace_enter $FUNCNAME

        launchCommand1="$VDLS -n \"Default FTP Site\""

        output="ACS:\"/opt/ap/acs/data/ftp\" ftpvol:\"/data/ftpvol\" images:\"/data/images\""

        tmp_file=`mktemp -t tempfile.XXXXXX`
        parsePrintout1="$CAT $tmp_file | $AWK '{print \$3}'"

        # Checking health on local node
        eval $launchCommand1 > $tmp_file
        cmdop=`eval $parsePrintout1`
        res=`$ECHO $cmdop | $SED 's/ "/:"/g' | $SED 's/ /\n/g' | $SORT -f`
        result=`$ECHO $res`
        checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
		flog "vdls for Default FTP site failed on local node"
        	rm $tmp_file
                return $exit_failure
        fi
	flog "vdls for Default FTP site succeeded on local node"

        flog "Healthcheck succeeded on local node"

        # Check if -l option is specified
        if [ $a_local == $FALSE ]; then # If no -l option is specified, then check health of remote node

                # Checking status of peer node
                check_peer_node_state
                if [ $? == 0 ]; then
                        # Launch command on remote node
                        ${SSH} -n $remote_node $launchCommand1 2>/dev/null > $tmp_file

                        cmdop=`eval $parsePrintout1`
                        res=`$ECHO $cmdop | $SED 's/ "/:"/g' | $SED 's/ /\n/g' | $SORT -f`
                        result=`$ECHO $res`

                        checkHealth "remote" $FUNCNAME
                        if [ $? == $exit_failure ]; then
				flog "vdls for Default FTP site failed on remote node"
        			rm $tmp_file
                                return $exit_failure
                        fi
			flog "vdls for Default FTP site succeeded on remote node"

                        flog "Healthcheck succeeded on remote node"
                else
                        flog "Remote node is not up. Health not checked on remote node"
                fi
        fi

        rm $tmp_file

	trace_leave $FUNCNAME
	return $exit_success
}

#---------------------------------------------------------
function IO_HCRS_6(){

	trace_enter $FUNCNAME

        tmp_file=`mktemp -t tempfile.XXXXXX`

        launchCommand1="$VDLS -n \"Default FTP Site\" -s SC-2-1"
        launchCommand2="$VDLS -n \"Default FTP Site\" -s SC-2-2"

      	parsePrintout="$CAT $tmp_file | $GREP \"Physical Path\" | $AWK '{print \$3}'"

        # Checking health on local node
        eval $launchCommand1 > $tmp_file
        cmdop=`eval $parsePrintout`
        res=`$ECHO $cmdop | $SED 's/ "/:"/g' | $SED 's/ /\n/g' | $SORT -f`
        result=`$ECHO $res`
        eval $launchCommand2 > $tmp_file
        cmdop=`eval $parsePrintout`
        res=`$ECHO $cmdop | $SED 's/ "/:"/g' | $SED 's/ /\n/g' | $SORT -f`
        op=`$ECHO $res`
        output=`$ECHO $op`

        checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
		flog "vdls comparison for Default FTP site succeeded on local node"
        	rm $tmp_file
                return $exit_failure
        fi
	flog "vdls comparison for Default FTP site succeeded on local node"

        flog "Healthcheck succeeded on local node"

        # Check if -l option is specified
        if [ $a_local == $FALSE ]; then # If no -l option is specified, then check health of remote node

                # Checking status of peer node
                check_peer_node_state
                if [ $? == 0 ]; then
                        # Launch command on remote node
                        ${SSH} -n $remote_node $launchCommand1 > $tmp_file
		        cmdop=`eval $parsePrintout`
		        res=`$ECHO $cmdop | $SED 's/ "/:"/g' | $SED 's/ /\n/g' | $SORT -f`
		        result=`$ECHO $res`
                        ${SSH} -n $remote_node $launchCommand2 > $tmp_file
	        	cmdop=`eval $parsePrintout`
		       	res=`$ECHO $cmdop | $SED 's/ "/:"/g' | $SED 's/ /\n/g' | $SORT -f`
        		op=`$ECHO $res`
                        output=`$ECHO $op`

                        checkHealth "remote" $FUNCNAME
                        if [ $? == $exit_failure ]; then
				flog "vdls comparison for Default FTP site failed on remote node"
        			rm $tmp_file
                                return $exit_failure
                        fi

			flog "vdls comparison for Default FTP site succeeded on remote node"
                        flog "Healthcheck succeeded on remote node"
                else
                        flog "Remote node is not up. Health not checked on remote node"
                fi
        fi

        rm $tmp_file

	trace_leave $FUNCNAME
	return $exit_success
}

#---------------------------------------------------------
function IO_HCRS_7(){

	trace_enter $FUNCNAME

        launchCommand1="$CAT /opt/ap/apos/conf/vsftpd/vsftpd.conf | $GREP anonymous_enable"
        launchCommand3="$CAT /opt/ap/apos/conf/vsftpd/vsftpd-APIO_1.conf | $GREP anonymous_enable"
        launchCommand4="$CAT /opt/ap/apos/conf/vsftpd/vsftpd-APIO_2.conf | $GREP anonymous_enable"

	output12="NO"
	output36="YES"

        tmp_file=`mktemp -t tempfile.XXXXXX`
        parsePrintout="$CAT $tmp_file | $CUT -d= -f 2"

        # Checking health on local node
	output=$output12
        eval $launchCommand1 > $tmp_file
        result=`eval $parsePrintout`
        checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
		flog "anonymous_enable check for Default FTP site failed on local node"
        	rm $tmp_file
                return $exit_failure
        fi
	flog "anonymous_enable check for Default FTP site succeeded on local node"

        output=$output36
        eval $launchCommand3 > $tmp_file
        result=`eval $parsePrintout`
        checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
		flog "anonymous_enable check for APIO_1 failed on local node"
        	rm $tmp_file
                return $exit_failure
        fi
	flog "anonymous_enable check for APIO_1 succeeded on local node"

        eval $launchCommand4 > $tmp_file
        result=`eval $parsePrintout`
        checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
		flog "anonymous_enable check for APIO_2 failed on local node"
        	rm $tmp_file
                return $exit_failure
        fi
	flog "anonymous_enable check for APIO_2 succeeded on local node"

        flog "Healthcheck succeeded"

        rm $tmp_file

	trace_leave $FUNCNAME
	return $exit_success
}

#---------------------------------------------------------
function IO_HCRS_8(){

	trace_enter $FUNCNAME

        launchCommand1="$CAT /etc/xinetd.d/vsftpd1 | $GREP log_type"
        launchCommand2="$CAT /etc/xinetd.d/vsftpd-APIO_1 | $GREP log_type"
        launchCommand3="$CAT /etc/xinetd.d/vsftpd-APIO_2 | $GREP log_type"

        output="FILE /dev/null"

        tmp_file=`mktemp -t tempfile.XXXXXX`
        parsePrintout="$CAT $tmp_file | $CUT -d= -f 2 | $SED 's/^ *//g'"

        # Checking health on local node
        eval $launchCommand1 > $tmp_file
        result=`eval $parsePrintout`
        if [ ! -z $result ]; then
		flog "log_type check for Default FTP site failed on local node"
        	rm $tmp_file
                return $exit_failure
        fi
	flog "log_type check for Default FTP site succeeded on local node"

        eval $launchCommand2 > $tmp_file
        result=`eval $parsePrintout`
        checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
		flog "log_type check for APIO_1 failed on local node"
        	rm $tmp_file
                return $exit_failure
        fi
	flog "log_type check for APIO_1 succeeded on local node"

        eval $launchCommand3 > $tmp_file
        result=`eval $parsePrintout`
        checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
		flog "log_type check for APIO_2 failed on local node"
        	rm $tmp_file
                return $exit_failure
        fi
	flog "log_type check for APIO_2 succeeded on local node"

        flog "Healthcheck succeeded on local node"

        # Check if -l option is specified
        if [ $a_local == $FALSE ]; then # If no -l option is specified, then check health of remote node

                # Checking status of peer node
                check_peer_node_state
                if [ $? == 0 ]; then
                        # Launch command on remote node
                        ${SSH} -n $remote_node $launchCommand1 2>/dev/null > $tmp_file
                        result=`eval $parsePrintout`
                        if [ ! -z $result ]; then
				flog "log_type check for Default FTP site failed on remote node"
        			rm $tmp_file
                                return $exit_failure
                        fi
			flog "log_type check for Default FTP site succeeded on remote node"
                        ${SSH} -n $remote_node $launchCommand2 2>/dev/null > $tmp_file
                        result=`eval $parsePrintout`
                        checkHealth "remote" $FUNCNAME
                        if [ $? == $exit_failure ]; then
				flog "log_type check for APIO_1 failed on remote node"
        			rm $tmp_file
                                return $exit_failure
                        fi
			flog "log_type check for APIO_1 succeeded on remote node"

                        ${SSH} -n $remote_node $launchCommand3 2>/dev/null > $tmp_file
                        result=`eval $parsePrintout`
                        checkHealth "remote" $FUNCNAME
                        if [ $? == $exit_failure ]; then
				flog "log_type check for APIO_2 failed on remote node"
        			rm $tmp_file
                                return $exit_failure
                        fi
			flog "log_type check for APIO_2 succeeded on remote node"

                        flog "Healthcheck succeeded on remote node"
                else
                        flog "Remote node is not up. Health not checked on remote node"
                fi
        fi

        rm $tmp_file

	trace_leave $FUNCNAME
	return $exit_success
}

#---------------------------------------------------------
function IO_HCRS_9(){

	trace_enter $FUNCNAME

        launchCommand1="$LS -alF /boot"
        launchCommand2="$LS -alF /boot/grub"
	mandatory_files_boot=".cluster.conf:grub/:initrd:lost+found/:serial.cmdline:vga.cmdline:vmlinuz"
	mandatory_files_grub="device.map:e2fs_stage1_5:menu.lst:stage1:stage2"

        tmp_file=`mktemp -t tempfile.XXXXXX`

        # Checking health on local node
        eval $launchCommand1 > $tmp_file
	cnt=$($ECHO $mandatory_files_boot | $GREP -o ":" | $WC -l)
	cnt=$(( $cnt + 1 ))

	for i in $(seq 1 $cnt)
	do
        	output=$($ECHO $mandatory_files_boot | $CUT -d: -f $i)
        	$CAT $tmp_file | $AWK '{print $9}' | $GREP -q $output
	        if [ $? == 0 ]; then
        		flog "File $output exists in /boot folder on local node"
	        else
        		flog "File $output does not exist in /boot folder on local node"
        		flog "Healthcheck failed on local node"
        		rm $tmp_file
                	return $exit_failure
	        fi
	done

        eval $launchCommand2 > $tmp_file
        cnt=$($ECHO $mandatory_files_grub | $GREP -o ":" | $WC -l)
        cnt=$(( $cnt + 1 ))

        for i in $(seq 1 $cnt)
        do
                output=$($ECHO $mandatory_files_grub | $CUT -d: -f $i)
                $CAT $tmp_file | $AWK '{print $9}' | $GREP -q $output
                if [ $? == 0 ]; then
                        flog "File $output exists in /boot/grub folder on local node"
                else
                        flog "File $output does not exist in /boot/grub folder on local node"
                        flog "Healthcheck failed on local node"
        		rm $tmp_file
                        return $exit_failure
                fi
        done


        flog "Healthcheck succeeded on local node"

        # Check if -l option is specified
        if [ $a_local == $FALSE ]; then # If no -l option is specified, then check health of remote node

                # Checking status of peer node
                check_peer_node_state
                if [ $? == 0 ]; then
                        # Launch command on remote node
                        ${SSH} -n $remote_node $launchCommand1 2>/dev/null > $tmp_file
		        for i in $(seq 1 $cnt)
		        do
                		output=$($ECHO $mandatory_files_boot | $CUT -d: -f $i)
                		$CAT $tmp_file | $AWK '{print $9}' | $GREP -q $output
		                if [ $? == 0 ]; then
                		        flog "File $output exists in /boot folder on remote node"
                		else
		                        flog "File $output does not exist in /boot folder on remote node"
                		        flog "Healthcheck failed on remote node"
        				rm $tmp_file
		                        return $exit_failure
                		fi
		        done

                        ${SSH} -n $remote_node $launchCommand2 2>/dev/null > $tmp_file
                        for i in $(seq 1 $cnt)
                        do
                                output=$($ECHO $mandatory_files_grub | $CUT -d: -f $i)
                                $CAT $tmp_file | $AWK '{print $9}' | $GREP -q $output
                                if [ $? == 0 ]; then
                                        flog "File $output exists in /boot/grub folder on remote node"
                                else
                                        flog "File $output does not exist in /boot/grub folder on remote node"
                                        flog "Healthcheck failed on remote node"
        				rm $tmp_file
                                        return $exit_failure
                                fi
                        done

		        flog "Healthcheck succeeded on remote node"

                else
                        flog "Remote node is not up. Health not checked on remote node"
                fi
        fi

        rm $tmp_file

	trace_leave $FUNCNAME
	return $exit_success
}

#---------------------------------------------------------
function IO_HCRS_10(){

	trace_enter $FUNCNAME

        launchCommand="$CAT /etc/fstab"
        output="/:/dev/pts:/dev/shm:/proc:/proc/bus/usb:/proc/fs/nfsd:/sys:/sys/kernel/config:/sys/kernel/debug:"
        tmp_file=`mktemp -t tempfile.XXXXXX`
        parsePrintout="$CAT $tmp_file | $AWK '{print \$2}' | $SORT | $TR \"\\n\" \":\""

        # Checking health on local node
        eval $launchCommand > $tmp_file
        result=`eval $parsePrintout`
        checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
                flog "File system table consistency check failed on local node"
        	rm $tmp_file
                return $exit_failure
        fi
        flog "File system table consistency check succeeded on local node"

        flog "Healthcheck succeeded on local node"

        # Check if -l option is specified
        if [ $a_local == $FALSE ]; then # If no -l option is specified, then check health of remote node

                # Checking status of peer node
                check_peer_node_state
                if [ $? == 0 ]; then
                        # Launch command on remote node
                        ${SSH} -n $remote_node $launchCommand 2>/dev/null > $tmp_file
                        result=`eval $parsePrintout`
                        checkHealth "remote" $FUNCNAME
                        if [ $? == $exit_failure ]; then
                                flog "File system table consistency check failed on remote node"
        			rm $tmp_file
                                return $exit_failure
                        fi
                        flog "File system table consistency check succeeded on remote node"
                        flog "Healthcheck succeeded on remote node"
                else
                        flog "Remote node is not up. Health not checked on remote node"
                fi
        fi

        rm $tmp_file

	trace_leave $FUNCNAME
	return $exit_success
}

#---------------------------------------------------------
#---------------------------------------------------------
#-----------TR HQ84451 ----------------------------------------------
#---------------------------------------------------------
function IO_HCRS_11(){

        trace_enter $FUNCNAME


        [[ ! -f $ssu_config_file || ! -s $ssu_config_file ]] && {
                flog "SSU_Quotas file doe not exist"
                return $exit_failure
        }

        tmp_file=`mktemp -t tempfile.XXXXXX`

        launchCommand="$REPQUOTA -g /data/ | $GREP USRGRP"

        quotaStatus=0
        # Checking health on local node
        eval $launchCommand > $tmp_file
        if [ $? -ne 0 ]; then
                flog "Command launch failed on local node"
                rm $tmp_file
                return $exit_failure
        fi
        [ ! -s $tmp_file ] && {
                flog "Empty temporary file generated."
                rm $tmp_file
                return $exit_failure
        }

        old_IFS=$IFS
        IFS=$'\n'
        for line in $($CAT $tmp_file | $TAIL -n +7)
        do
                grp_name=`echo $line | awk '{print $1}'`
		[ -z $grp_name ] && {
			flog "Empty Group name found"
			quotaStatus=1
			break
		}

                grp_folder=${grp_name:0:3}
                if [ $grp_folder == ACS ]; then
                        continue;
                fi
                quota=`echo $line | awk '{print $5}'`
                [ -z $quota ] && {
                        flog "Quota for $grp_name $grp_folder not found"
                        quotaStatus=1
                        break
                }

                ssu_line=`cat $ssu_config_file | grep ^BSC | grep $grp_folder`
                if [ ! -z "$ssu_line" ];then
                        size=`echo $ssu_line | $AWK -F, '{print $4}'`
                        [ -z $size ] && {
                                flog "size for $grp_name $grp_folder not found"
                                quotaStatus=1
                                break
                        }
                        hard_limit=`echo $ssu_line | $AWK -F, '{print $5}'`
                        [ -z $hard_limit ] && {
                                flog "hard_limit for $grp_name $grp_folder not found"
                                quotaStatus=1
                                break
                        }

                        ssu_quota1=`echo "scale=1; ($size-($hard_limit*$size/100))*1024" | bc -l`
                        [ ! -z $ssu_quota1 ] && ssu_quota=$( echo "$ssu_quota1" | cut -d . -f1)
                        #echo ssu quota, repquota: $grp_folder - $ssu_quota, $quota
                        if [ "$quota" = "$ssu_quota" ]; then
                                flog "Healthcheck succeeded in quota verification for $grp_folder folder"
                        else
                                flog "Healthcheck failed in quota verification for $grp_folder folder"
                                quotaStatus=1
                                break
                        fi
                else
                        flog "Quota for $grp_folder not defined in SSU BSC"
                fi
        done
        if [ $quotaStatus == 0 ]; then
                flog "Healthcheck succeeded"
        else
                flog "Healthcheck failed"
                rm $tmp_file
                return $exit_failure
        fi
        IFS=$old_IFS

        rm $tmp_file

        trace_leave $FUNCNAME
        return $exit_success
}



#---------------------------------------------------------
function IO_HCRS_12(){

        trace_enter $FUNCNAME

        tmp_file=`mktemp -t tempfile.XXXXXX`

        launchCommand="$REPQUOTA -g /data/"

        # Checking health on local node
        $launchCommand > $tmp_file
	if [ $? -ne 0 ]; then
		flog "Command launch failed on local node"
        	rm $tmp_file
		return $exit_failure
	fi
	result=0
	for i in $($CAT $tmp_file | $AWK '{print $5}' | $TAIL -n +6)
	do
		quota=$(( $i / 1024 ))
        	result=$(( $result + $quota ))
	done

	result=$(( $result / 1000 ))
	output=`$FDISK -l 2> /dev/null | $GREP md0 | $AWK '{print $3}'`
        if [ `echo "$result >= $output" | bc` -eq 1 ]; then
                flog "SSU quota limit exceeded the data disk physical size"
        	rm $tmp_file
                return $exit_failure
        fi
        flog "Check to verify that SSU quota limit does not exceed data disk physical size succeeded on local node"
        flog "Healthcheck succeeded"

        rm $tmp_file

        trace_leave $FUNCNAME
        return $exit_success
}

#---------------------------------------------------------

function IO_HCRS_13(){
  trace_enter $FUNCNAME
#  HW_TYPE=$(GetHwType)
#  if [[ $HW_TYPE == 'VM' ]] ;then
#    flog "SMART Monitor tool is not supported for vAPG"
#    trace_leave $FUNCNAME
#    return $exit_success
#  fi
  tmp_file=`mktemp -t tempfile.XXXXXX`
  output="OK"
  gep_type=`check_gep_info`
  disk_type=`check_disk_info`
  if [ $gep_type -eq 1 -o $gep_type -eq 2 ]; then
    parsePrintout1="$CAT $tmp_file | $SED -e 's/ //g'"
    parsePrintout2="$CAT $tmp_file | $GREP -o "Error""
    output=`$RAIDMGMT -i | $GREP -o "sd" |$WC -l`
    if [ $output -ge 0 ];then
      launchCommand="$RAIDMGR -a"
      eval $launchCommand > $tmp_file
      result=`eval $parsePrintout1`
      if [ $result == BothDisksareNOTOK ];then
        flog "SMART check failed on diskA and diskB"
        rm $tmp_file
        return $exit_failure
      elif [ $result == BothDisksareOK ];then
        flog "SMART check succeeded on diskA and diskB"
        flog "Healthcheck succeeded"
        rm $tmp_file
        trace_leave $FUNCNAME
        return $exit_success
      elif [ $result == diskBNOTOK ];then
        if [ $output -eq 2 ];then
          flog "SMART check failed on diskB"
          rm $tmp_file
          return $exit_failure
        else
          flog "diskB is not available"
          rm $tmp_file
          return $exit_failure
        fi
      elif [ $result == diskANOTOK ];then
        if [ $output -eq 2 ];then
          flog "SMART check failed on diskA"
					rm $tmp_file
          return $exit_failure
        else
          flog "diskA is not available"
          rm $tmp_file
          return $exit_failure
        fi
      fi
      result=`eval $parsePrintout2`
      if [[ ! -z $result ]]; then
        if [ $result == Error ];then
            flog "SMART check execution failed on diskA and diskB"
            rm $tmp_file
            return $exit_failure
        fi
      fi
    else
      flog "diskA and diskB are not available"
      rm $tmp_file
      return $exit_failure
    fi
  elif [ $gep_type -eq 4 -o $gep_type -eq 5 -o $gep_type -eq 7 ]; then
    launchCommand4="$SMARTCTL --all /dev/eri_disk"
    launchCommand5="$SMARTCTL --all /dev/eri_diskC"
    launchCommand6="$SMARTCTL --all /dev/eri_diskD"
    status="PASSED"
    parsePrintout="$CAT $tmp_file | $AWK -F: '{if(\$1==\"SMART Health Status\") print \$2}' | $SED 's/[[:space:]]*//g'"
    parsePrintout1="$CAT $tmp_file | $AWK -F: '{if(\$1==\"SMART overall-health self-assessment test result\") print \$2}'| $SED 's/[[:space:]]*//g'"
    output=$status
     # value modified to implement HW decoupling
    if [ $disk_type -eq 1 ]; then
      # Checking health on local node
      eval $launchCommand4 > $tmp_file
      result=`eval $parsePrintout1`
      checkHealth "local" $FUNCNAME
      if [ $? == $exit_failure ]; then
        flog "SMART check failed for /dev/eri_disk on local node"
        rm $tmp_file
        return $exit_failure
      fi
        flog "SMART check succeeded for /dev/eri_disk on local node"
        flog "Healthcheck succeeded on local node"
    elif [ $disk_type -eq 3 ]; then
      # Checking health on local node
      eval $launchCommand4 > $tmp_file
      result=`eval $parsePrintout1`
      checkHealth "local" $FUNCNAME
			if [ $? == $exit_failure ]; then
        flog "SMART check failed for /dev/eri_disk on local node"
        rm $tmp_file
        return $exit_failure
      fi
      flog "SMART check succeeded for /dev/eri_disk on local node"
      eval $launchCommand5 > $tmp_file
      result=`eval $parsePrintout1`
      checkHealth "local" $FUNCNAME
      if [ $? == $exit_failure ]; then
        flog "SMART check failed for /dev/eri_diskC on local node"
        rm $tmp_file
        return $exit_failure
      fi
      flog "SMART check succeeded for /dev/eri_diskC on local node"
      eval $launchCommand6 > $tmp_file
      result=`eval $parsePrintout1`
      checkHealth "local" $FUNCNAME
      if [ $? == $exit_failure ]; then
        flog "SMART check failed for /dev/eri_diskD on local node"
        rm $tmp_file
        return $exit_failure
      fi
      flog "SMART check succeeded for /dev/eri_diskD on local node"
      flog "Healthcheck succeeded on local node"
    elif [ $disk_type -eq 2 ]; then
      # Checking health on local node
      eval $launchCommand4 > $tmp_file
      result=`eval $parsePrintout1`
      checkHealth "local" $FUNCNAME
                        if [ $? == $exit_failure ]; then
        flog "SMART check failed for /dev/eri_disk on local node"
        rm $tmp_file
        return $exit_failure
      fi
      flog "SMART check succeeded for /dev/eri_disk on local node"
      eval $launchCommand5 > $tmp_file
      result=`eval $parsePrintout1`
      checkHealth "local" $FUNCNAME
      if [ $? == $exit_failure ]; then
        flog "SMART check failed for /dev/eri_diskC on local node"
        rm $tmp_file
        return $exit_failure
      fi
      flog "SMART check succeeded for /dev/eri_diskC on local node"
      flog "Healthcheck succeeded on local node"
    fi

    # Check if -l option is specified
      if [ $a_local == $FALSE ]; then # If no -l option is specified, then check health of remote node
        # Checking status of peer node
        check_peer_node_state
        if [ $? == 0 ]; then
        # Launch command on remote node
         # value modified to implement HW decoupling
          if [ $disk_type -eq 1 ];  then
            ${SSH} -n $remote_node $launchCommand4 2>/dev/null > $tmp_file
            result=`eval $parsePrintout1`
            checkHealth "remote" $FUNCNAME
            if [ $? == $exit_failure ]; then
              flog "SMART check failed for /dev/eri_disk on remote node"
              rm $tmp_file
              return $exit_failure
            fi
            flog "SMART check succeeded for /dev/eri_disk on remote node"
            flog "Healthcheck succeeded on remote node"
          elif [ $disk_type -eq 3 ];  then
            ${SSH} -n $remote_node $launchCommand4 2>/dev/null > $tmp_file
            result=`eval $parsePrintout1`
						checkHealth "remote" $FUNCNAME
            if [ $? == $exit_failure ]; then
              flog "SMART check failed for /dev/eri_disk on remote node"
              rm $tmp_file
              return $exit_failure
            fi
            flog "SMART check succeeded for /dev/eri_disk on remote node"
            ${SSH} -n $remote_node $launchCommand5 2>/dev/null > $tmp_file
            result=`eval $parsePrintout1`
            checkHealth "remote" $FUNCNAME
            if [ $? == $exit_failure ]; then
              flog "SMART check failed for /dev/eri_diskC on remote node"
              rm $tmp_file
              return $exit_failure
            fi
            flog "SMART check succeeded for /dev/eri_diskC on remote node"
            ${SSH} -n $remote_node $launchCommand6 2>/dev/null > $tmp_file
            result=`eval $parsePrintout1`
            checkHealth "remote" $FUNCNAME
            if [ $? == $exit_failure ]; then
              flog "SMART check failed for /dev/eri_diskD on remote node"
              rm $tmp_file
              return $exit_failure
            fi
            flog "SMART check succeeded for /dev/eri_diskD on remote node"
            flog "Healthcheck succeeded on remote node"
          elif [ $disk_type -eq 2 ];  then
            ${SSH} -n $remote_node $launchCommand4 2>/dev/null > $tmp_file
            result=`eval $parsePrintout1`
                                                checkHealth "remote" $FUNCNAME
            if [ $? == $exit_failure ]; then
              flog "SMART check failed for /dev/eri_disk on remote node"
              rm $tmp_file
              return $exit_failure
            fi
            flog "SMART check succeeded for /dev/eri_disk on remote node"
            ${SSH} -n $remote_node $launchCommand5 2>/dev/null > $tmp_file
            result=`eval $parsePrintout1`
            checkHealth "remote" $FUNCNAME
            if [ $? == $exit_failure ]; then
              flog "SMART check failed for /dev/eri_diskC on remote node"
              rm $tmp_file
              return $exit_failure
            fi
            flog "SMART check succeeded for /dev/eri_diskC on remote node"
            flog "Healthcheck succeeded on remote node"
          else
          flog "Remote node is not up. Health not checked on remote node"
          fi
        fi
      fi
        rm $tmp_file
	trace_leave $FUNCNAME
        return $exit_success
  fi
}

#---------------------------------------------------------
function IO_HCRS_14(){

	trace_enter $FUNCNAME
    tmp_file=`mktemp -t tempfile.XXXXXX`

    # Check the data disk replication type
    DD_REP_TYP=$(immlist -a dataDiskReplicationType axeFunctionsId=1 2>/dev/null | awk -F'=' '{print $2}')

    if [ $DD_REP_TYP -eq 1 ]; then
        launchCommand="$CAT /proc/mdstat"
	output_up="[2/2] [UU]"

	parsePrintout3="$CAT $tmp_file | $GREP -A1 raid1 | $TAIL -1 | $AWK '{print \$(NF-1),\$NF}'"

       		  $launchCommand > $tmp_file
        result=`eval $parsePrintout3`
	output=$output_up
        checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
                flog "Check to verify RAID state of both disks failed on local node"
        	rm $tmp_file
                return $exit_failure
        fi
        flog "Check to verify RAID state of both disks succeeded on local node"
        flog "Healthcheck succeeded"
 	  elif [ $DD_REP_TYP -eq 2 ]; then
         	launchCommand="$RAIDMGR -s"
                parsePrintout="cat $tmp_file"
                output_status="UP"

      		# Checking raidmgr status on local node
        	eval $launchCommand > $tmp_file
        	result=$( eval $parsePrintout); result=$(echo $result)
        	output=$output_status
        	checkHealth "local" $FUNCNAME
        	[ $? == $exit_failure ] && {
                     flog "Data Disks are not mirrored"
                     rm $tmp_file
                     return $exit_failure
        	}
        	flog "DRBD disk check success"
           fi
    rm $tmp_file
    trace_leave $FUNCNAME
    return $exit_success
  
	
}


#---------------------------------------------------------
function IO_HCRS_15(){

	trace_enter $FUNCNAME

        launchCommand1="$CMW_STATUS -v node"
        launchCommand2="$PRCSTATE -l"

        output1="UNLOCKED"
        output2="ENABLED"
        output3="up and working"

        tmp_file=`mktemp -t tempfile.XXXXXX`
        parsePrintout1="$CAT $tmp_file | $GREP AdminState | $CUT -d= -f2 | $CUT -d'(' -f1 | $HEAD -1"
        parsePrintout2="$CAT $tmp_file | $GREP AdminState | $CUT -d= -f2 | $CUT -d'(' -f1 | $TAIL -1"
        parsePrintout3="$CAT $tmp_file | $GREP OperState | $CUT -d= -f2 | $CUT -d'(' -f1 | $HEAD -1"
        parsePrintout4="$CAT $tmp_file | $GREP OperState | $CUT -d= -f2 | $CUT -d'(' -f1 | $TAIL -1"
        parsePrintout5="$CAT $tmp_file | $GREP active | $GREP -o \"up and working\""
        parsePrintout6="$CAT $tmp_file | $GREP passive | $GREP -o \"up and working\""

        # Checking health on local node
        eval $launchCommand1 > $tmp_file

        output=$output1
        result=`eval $parsePrintout1`
        checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
		flog "cmw-status check for SC-2-1 AdminState failed on local node"
        	rm $tmp_file
                return $exit_failure
        fi
	flog "cmw-status check for SC-2-1 AdminState succeeded on local node"

        result=`eval $parsePrintout2`
        checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
		flog "cmw-status check for SC-2-2 AdminState failed on local node"
        	rm $tmp_file
                return $exit_failure
        fi
	flog "cmw-status check for SC-2-2 AdminState succeeded on local node"

        output=$output2
        result=`eval $parsePrintout3`
        checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
		flog "cmw-status check for SC-2-1 OperState failed on local node"
        	rm $tmp_file
                return $exit_failure
        fi
	flog "cmw-status check for SC-2-1 OperState succeeded on local node"
        result=`eval $parsePrintout4`
        checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
		flog "cmw-status check for SC-2-2 OperState failed on local node"
        	rm $tmp_file
                return $exit_failure
        fi
	flog "cmw-status check for SC-2-2 OperState succeeded on local node"

        eval $launchCommand2 > $tmp_file

        output=$output3
        result=`eval $parsePrintout5`
        checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
		flog "prcstate for active node failed on local node"
        	rm $tmp_file
                return $exit_failure
        fi
	flog "prcstate for active node succeeded on local node"

        result=`eval $parsePrintout6`
        checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
		flog "prcstate for passive node failed on local node"
        	rm $tmp_file
                return $exit_failure
        fi
	flog "prcstate for passive node succeeded on local node"
        flog "Healthcheck succeeded"

        rm $tmp_file

	trace_leave $FUNCNAME
	return $exit_success
}

#---------------------------------------------------------
function IO_HCRS_16(){

	trace_enter $FUNCNAME

        launchCommand="$CMW_STATUS comp"
#       output="Status OK"
        output=0
        tmp_file=`mktemp -t tempfile.XXXXXX`
        parsePrintout="$CAT $tmp_file"

        # Checking health on local node
        eval $launchCommand > $tmp_file
	result=$?
#        result=`eval $parsePrintout`
        checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
		flog "cmw-status comp failed on local node"
        	rm $tmp_file
                return $exit_failure
        fi
	flog "cmw-status comp succeeded on local node"

        flog "Healthcheck succeeded"

        rm $tmp_file

	trace_leave $FUNCNAME
	return $exit_success
}

#---------------------------------------------------------
function IO_HCRS_17(){
	if [ $a_local == $TRUE ]; then # # If '-l' option is specified, rule should not be executed.
        	return $exit_success
	else

		trace_enter $FUNCNAME
        launchCommand1="drbdadm cstate drbd0"
        launchCommand2="drbdadm role drbd0"
        launchCommand3="drbdadm dstate drbd0"

        output1="Connected"
        output2="Primary"
        output3="Secondary"
        output4="UpToDate/UpToDate"

        result=`eval $launchCommand1`
	output=$output1
        checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
		flog "drbd check for cs failed on local node"
                return $exit_failure
        fi
	flog "drbd check for cs succeeded on local node"

        result=`eval $launchCommand2`
	output=$output2
        checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
		output=$output3
        	checkHealth "local" $FUNCNAME
	        if [ $? == $exit_failure ]; then
			flog "drbd check for ro failed on local node"
        	        return $exit_failure
	        fi
	fi
	flog "drbd check for ro succeeded on local node"

        result=`eval $launchCommand3`
	output=$output4
        checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
		flog "drbd check for ds failed on local node"
                return $exit_failure
        fi
	flog "drbd check for ds succeeded on local node"
        flog "Healthcheck succeeded"


	trace_leave $FUNCNAME
	return $exit_success
	fi
}

#---------------------------------------------------------
function IO_HCRS_18(){

	trace_enter $FUNCNAME

        tmp_file=`mktemp -t tempfile.XXXXXX`

        launchCommand1="$HOSTNAME"
        launchCommand2="$CAT /etc/opensaf/node_name"

        parsePrintout="$CAT $tmp_file"

        # Checking health on local node
        eval $launchCommand1 > $tmp_file
        result=`eval $parsePrintout`
        eval $launchCommand2 > $tmp_file
        output=`eval $parsePrintout`
        checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
                flog "AP hostname did not match with hostname in CoreMW on local node"
        	rm $tmp_file
                return $exit_failure
        fi
        flog "AP hostname matched with hostname in CoreMW on local node"

        flog "Healthcheck succeeded on local node"

        # Check if -l option is specified
        if [ $a_local == $FALSE ]; then # If no -l option is specified, then check health of remote node

                # Checking status of peer node
                check_peer_node_state
                if [ $? == 0 ]; then
                        # Launch command on remote node
                        ${SSH} -n $remote_node $launchCommand1 2>/dev/null > $tmp_file
                        result=`eval $parsePrintout`
                        ${SSH} -n $remote_node $launchCommand2 2>/dev/null > $tmp_file
                        output=`eval $parsePrintout`
                        checkHealth "remote" $FUNCNAME
                        if [ $? == $exit_failure ]; then
                                flog "AP hostname did not match with hostname on remote node"
        			rm $tmp_file
                                return $exit_failure
                        fi

                        flog "AP hostname matched with hostname on remote node"
                        flog "Healthcheck succeeded on remote node"
                else
                        flog "Remote node is not up. Health not checked on remote node"
                fi
        fi

        rm $tmp_file

	trace_leave $FUNCNAME
	return $exit_success
}

#---------------------------------------------------------
function IO_HCRS_19(){

	trace_enter $FUNCNAME

        launchCommand1="$ALIST -s A1"
        launchCommand2="$ALIST -s A2"
        output="No match found."
        tmp_file=`mktemp -t tempfile.XXXXXX`
	tmp_file2=`mktemp -t tempfile.XXXXXX`
	tmp_file3=`mktemp -t tempfile.XXXXXX`
        parsePrintout="$CAT $tmp_file"

        # Checking health on local node
        eval $launchCommand1 > $tmp_file
        result=`eval $parsePrintout`
        checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
                alarm=$($CAT $tmp_file | $GREP "Alarm Identifier" -A 2 | $CUT -d " " -f 1  | $GREP -v ^Alarm | $GREP [0-9])
                for i in $alarm
                do
			#Checking if ceasing alarms are present or not
                        launchCommand3="$ALIST -i $i"
                        eval $launchCommand3 > $tmp_file2
                        $CAT $tmp_file2 | $GREP -w CEASING >/dev/null
                        if [ $? == 0 ]; then
                                flog "Ceasing alarm exists with alarm identifier $i"
                        else
              	flog "One or more A1 alarms active on local node"
                                rm $tmp_file $tmp_file2 $tmp_file3
                                return $exit_failure
                        fi
                done

        fi
        flog "No severe A1 alarms active on local node"

        eval $launchCommand2 > $tmp_file
        result=`eval $parsePrintout`
        checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
                alarm=$($CAT $tmp_file | $GREP "Alarm Identifier" -A 2 | $CUT -d " " -f 1  | $GREP -v ^Alarm | $GREP [0-9])
                for i in $alarm
                do
			#Checking if ceaseing alarms are present or not
                        launchCommand3="$ALIST -i $i"
                        eval $launchCommand3 > $tmp_file3
                        $CAT $tmp_file3 | $GREP -w CEASING >/dev/null
                        if [ $? == 0 ]; then
                                flog "Ceasing alarm exists with alarm identifier $i"
                        else
              	flog "One or more A2 alarms active on local node"
                                rm $tmp_file $tmp_file2 $tmp_file3
                                return $exit_failure
                        fi
                done
        fi
        flog "No severe A2 alarms active on local node"
        flog "Healthcheck succeeded on local node"

        # Check if -l option is specified
        if [ $a_local == $FALSE ]; then # If no -l option is specified, then check health of remote node

                # Checking status of peer node
                check_peer_node_state
                if [ $? == 0 ]; then
                        # Launch command on remote node
                        ${SSH} -n $remote_node $launchCommand1 2>/dev/null > $tmp_file
                        result=`eval $parsePrintout`
                        checkHealth "remote" $FUNCNAME
                        if [ $? == $exit_failure ]; then
                                alarm=$($CAT $tmp_file | $GREP "Alarm Identifier" -A 2 | $CUT -d " " -f 1  | $GREP -v ^Alarm | $GREP [0-9])
                                for i in $alarm
                                do
                                        #Checking if ceasing alarms are present
					launchCommand3="$ALIST -i $i"
                                        ${SSH} -n $remote_node $launchCommand3 2>/dev/null > $tmp_file2
                                        $CAT $tmp_file2 | $GREP -w CEASING >/dev/null
                                        if [ $? == 0 ]; then
                                                flog "Ceasing alarm exists with alarm identifier $i on remote node"
                                        else
                		flog "One or more A1 alarms active on remote node"
                                                rm $tmp_file $tmp_file2 $tmp_file3
                                                return $exit_failure
                                        fi
                                done

                        fi
        		flog "No severe A1 alarms active on remote node"

                        ${SSH} -n $remote_node $launchCommand2 2>/dev/null > $tmp_file
                        result=`eval $parsePrintout`
                        checkHealth "remote" $FUNCNAME
                        if [ $? == $exit_failure ]; then
                                alarm=$($CAT $tmp_file | $GREP "Alarm Identifier" -A 2 | $CUT -d " " -f 1  | $GREP -v ^Alarm | $GREP [0-9])
                                for i in $alarm
                                do
					#Checking if ceasing alarms are present
                                        launchCommand3="$ALIST -i $i"
                                        ${SSH} -n $remote_node $launchCommand3 2>/dev/null > $tmp_file2
                                        $CAT $tmp_file2 | $GREP -w CEASING >/dev/null
                                        if [ $? == 0 ]; then
                                                flog "Ceasing alarm exists with alarm identifier $i on remote node"
                                        else
                		flog "One or more A2 alarms active on remote node"
                                                rm $tmp_file $tmp_file2 $tmp_file3
                                                return $exit_failure
                                        fi
                                done

                        fi
        		flog "No severe A2 alarms active on remote node"
                        flog "Healthcheck succeeded on remote node"
                else
                        flog "Remote node is not up. Health not checked on remote node"
                fi
        fi

        rm $tmp_file $tmp_file2 $tmp_file3

	trace_leave $FUNCNAME
	return $exit_success
}

#---------------------------------------------------------
#-----------------HR94126---------------------------------
#--------------------------------------------------------
function IO_HCRS_20(){
 	trace_enter $FUNCNAME
        # For mml commands to work
        export PORT=4422
	if [ $a_local == $TRUE ]; then # If '-l' option is specified, rule should not be executed.
		trace_leave $FUNCNAME
        	return $exit_success
	else
	        tmp_file1=`mktemp -t tempfile.XXXXXX`
                echo APAMP > $tmp_file1
	        launchCommand="$MML -f $CAT $tmp_file1"
        output1="ACTIVE"
        output2="PASSIVE"
        tmp_file=`mktemp -t tempfile.XXXXXX`
        getColumn="$CAT $tmp_file | $GREP -i status | $SED 's/ \+ /\n/g' | $GREP -ni status | $CUT -d: -f1"
        column_no=
        parsePrintout="cat $tmp_file | $AWK -v v=$column_no '{print \$v}' | $SED '/^$/d'"

        # Checking health on local node
        eval $launchCommand 2>/dev/null > $tmp_file
        if [ $? -ne 0 ]; then
                flog "Command launch failed on local node"
        	rm $tmp_file
	        	rm $tmp_file1
                return $exit_failure
        fi
        column_no=`eval $getColumn`
        result=`cat $tmp_file | $AWK -v v=$column_no '{print \$v}' | $SED '/^$/d' | $TAIL -n +2`
	for i in $(echo $result)
	do
		if [[ $i == $output1 || $i == $output2 ]]; then
			flog "AP maintenance data has $i state"
		else
			flog "AP maintenance data has $i state"
			flog "Check for normal state of AP maintenance data failed on local node"
        		rm $tmp_file
	        		rm $tmp_file1
			return $exit_failure
		fi
	done
        flog "Healthcheck succeeded"

        	rm $tmp_file $tmp_file1 
       

		trace_leave $FUNCNAME
		return $exit_success
	fi
}
#---------------------------------------------------------         
#----------TR HQ85059: Checking only administrator -----------------------------------------------         
#---------------------------------------------------------
#---------------------------------------------------------
function IO_HCRS_21(){

	trace_enter $FUNCNAME

        tmp_file=`mktemp -t tempfile.XXXXXX`

        launchCommand1="$CAT /cluster/etc/passwd"
        launchCommand2="$LISTTSUSER"

        parsePrintout1="$CAT $tmp_file | grep -ci tsadmin"
        parsePrintout2="$CAT $tmp_file | grep -vi \"No troubleshooting user\" | wc -l"

        # Checking health on local node
        eval $launchCommand1 > $tmp_file
        result=`eval $parsePrintout1`
	if [ $result -lt 1 ]; then
                flog "Administrator user does not exist on local node"
        	rm $tmp_file
                return $exit_failure
        fi
        flog "Administrator user exists on local node"

#        eval $launchCommand2 > $tmp_file
#        result=`eval $parsePrintout2`
#	if [ $result -lt 1 ]; then
#                flog "Troubleshooting user does not exist on local node"
#        	rm $tmp_file
#                return $exit_failure
#        fi
#        flog "Troubleshooting user exists on local node"

        flog "Healthcheck succeeded on local node"

        # Check if -l option is specified
        if [ $a_local == $FALSE ]; then # If no -l option is specified, then check health of remote node

                # Checking status of peer node
                check_peer_node_state
                if [ $? == 0 ]; then
                        # Launch command on remote node
                        ${SSH} -n $remote_node $launchCommand1 2>/dev/null > $tmp_file
                        result=`eval $parsePrintout1`
			if [ $result -lt 1 ]; then
		                flog "Administrator user does not exist on remote node"
        			rm $tmp_file
                		return $exit_failure
		        fi
		        flog "Administrator user exists on remote node"

#                        ${SSH} -n $remote_node $launchCommand2 2>/dev/null > $tmp_file
#                        result=`eval $parsePrintout2`
#			if [ $result -lt 1 ]; then
#		                flog "Troubleshooting user does not exist on remote node"
#        			rm $tmp_file
#                		return $exit_failure
#		        fi
#		        flog "Troubleshooting user exists on remote node"
                        flog "Healthcheck succeeded on remote node"
                else
                        flog "Remote node is not up. Health not checked on remote node"
                fi
        fi

        rm $tmp_file

	trace_leave $FUNCNAME
	return $exit_success
}

#---------------------------------------------------------
function IO_HCRS_22(){

	if [ $a_local == $TRUE ]; then # If '-l' option is specified, rule should not be executed.
	        return $exit_success
	else
	trace_enter $FUNCNAME

        tmp_file=`mktemp -t tempfile.XXXXXX`

        launchCommand="$DATE"

        parsePrintout="$CAT $tmp_file | $AWK '{\$4=NULL; print}'"
        parsePrintout1="$CAT $tmp_file | $AWK '{print \$4}' | $AWK -F: '{print \$1\$2}'"

        # Checking health on local node
        eval $launchCommand > $tmp_file
        result=`eval $parsePrintout`
	# Checking status of peer node
       	check_peer_node_state
        if [ $? == 0 ]; then
       		# Launch command on remote node
               	${SSH} -n $remote_node $launchCommand 2>/dev/null > $tmp_file
	        output=`eval $parsePrintout`
       		checkHealth "local" $FUNCNAME
	        if [ $? == $exit_failure ]; then
       		        flog "Date verification on AP nodes failed"
        		rm $tmp_file
               		return $exit_failure
	        fi	
       		flog "Date verification on AP nodes succeeded"

		# Verification of time 
       		eval $launchCommand > $tmp_file
	        result=`eval $parsePrintout1`
		# Launch command on remote node
        	${SSH} -n $remote_node $launchCommand 2>/dev/null > $tmp_file
        	output=`eval $parsePrintout1`

		# TR HQ60276 - To convert numbers to decimal
                output=`echo $output | bc`
                result=`echo $result | bc`
		# A discrepancy of 5 minutes is acceptable
		if [[ $output -le $(( $result + 5 )) || $output -ge $(( $result - 5 )) ]]; then
			flog "Time verification on AP nodes succeeded"
		else
			flog "Time verification on AP nodes failed"
        		rm $tmp_file
               		return $exit_failure
		fi
	else	
		flog "Remote node is down. Date on remote node not checked."
        	rm $tmp_file
                return $exit_failure
	fi

        eval $launchCommand > $tmp_file
        result=`eval $parsePrintout1`
        if [ $? == $exit_failure ]; then
                        flog "Date and time verification on AP nodes failed"
        		rm $tmp_file
                        return $exit_failure
                fi
                flog "Date and time verification on AP nodes succeeded"
                flog "Healthcheck succeeded on local node"

        rm $tmp_file

	trace_leave $FUNCNAME
	return $exit_success
	fi
}

#---------------------------------------------------------
function IO_HCRS_23(){

	trace_enter $FUNCNAME

        tmp_file=`mktemp -t tempfile.XXXXXX`

        launchCommand="$DATE +%y%m%d"
        launchCommand1="$DATE +%H%M%S"
        launchCommand2="$MML caclp"

        parsePrintout="$CAT $tmp_file"
        parsePrintout1="$CAT $tmp_file | $GREP -w -A1 DATE | $TAIL -1 | $AWK '{print \$1}'"
        parsePrintout2="$CAT $tmp_file | $GREP -w -A1 DATE | $TAIL -1 | $AWK '{print \$2}'"

        # Checking health on local node
	eval $launchCommand > $tmp_file
        ap_date=`eval $parsePrintout`
	eval $launchCommand1 > $tmp_file
        ap_time=`eval $parsePrintout`

        check_cp_state
        if [ $? -ne 0 ]; then
                flog "Failed to retrieve CP information"
        	rm $tmp_file
                return $exit_failure
        fi
        if [ $singleCP == 1 ]; then
                eval $launchCommand2  2>/dev/null > $tmp_file
		if [ $? -ne 0 ]; then
			flog "Command launch failed"
        		rm $tmp_file
			return $exit_failure
		fi
                cp_date=`eval $parsePrintout1`
                eval $launchCommand2  2>/dev/null > $tmp_file
		if [ $? -ne 0 ]; then
			flog "Command launch failed"
        		rm $tmp_file
			return $exit_failure
		fi
                cp_time=`eval $parsePrintout2`

	        result=$ap_date
		output=$cp_date
        	checkHealth "local" $FUNCNAME
	        if [ $result -le 0 ]; then
        	        flog "AP-CP date check failed."
        		rm $tmp_file
                	return $exit_failure
	        fi
		flog "AP-CP date check passed."

	        result=$ap_time
		output=$cp_time
        	checkHealth "local" $FUNCNAME
	        if [ $result -le 0 ]; then
        	        flog "AP-CP time check failed."
        		rm $tmp_file
                	return $exit_failure
	        fi
		flog "AP-CP time check passed."
        fi

        if [ $multiCP == 1 ]; then
		echo ""
		flog "Date and time check for Multi CP not implemented"
	fi
        
	flog "Check to verify AP-CP date and time succeeded"
        flog "Healthcheck succeeded"

        rm $tmp_file

	trace_leave $FUNCNAME
	return $exit_success
}

#---------------------------------------------------------
function IO_HCRS_24(){

	trace_enter $FUNCNAME

        #tmp_file=`mktemp -t tempfile.XXXXXX`
	#ldap_user=$1
        #Pid=$$
        #ppid=$PPID
        #gpid=`echo $(ps -p $ppid -o ppid) | $AWK '{print $2}'`
        #user=`ps -p $gpid -o user | $GREP -vw USER`

        ##launchCommand="$LDAPSEARCH -xLLL -b \"dc=example,dc=com\" \"uid=cpadmin\" shadowMax | grep shadowMax"
        #launchCommand="$LDAPSEARCH -xLLL -b \"dc=example,dc=com\" \"uid=$user\" shadowExpire | grep shadowExpire"
        #parsePrintout="$CAT $tmp_file | $CUT -d: -f2 | $SED 's/^[[:space:]]//g'"

       	#eval $launchCommand 2> /dev/null > $tmp_file
	#if [ $? -ne 0 ]; then
	#	flog "Command launch failed on node"	
        # 	rm $tmp_file
	#	return $exit_failure
	#fi
        #result=`eval $parsePrintout`
        #if [ $result -le 0 ]; then
       	#        flog "Check to verify account expiry for $user failed."
        #	rm $tmp_file
        #       	return $exit_failure
        #fi
       	#flog "Check to verify account expiry for $user succeeded."
        #flog "Healthcheck succeeded"
	
        #rm $tmp_file

	trace_leave $FUNCNAME
	return $exit_success
}

#---------------------------------------------------------
function IO_HCRS_25(){

	trace_enter $FUNCNAME

        tmp_file=`mktemp -t tempfile.XXXXXX`

        launchCommand="$SERVICE watchdogd status"

        parsePrintout="$CAT $tmp_file | $AWK '{print \$NF}' | $SED 's/[.]//g'"
	output="running"

        # Checking health on local node
        eval $launchCommand > $tmp_file
        result=`eval $parsePrintout`
        checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
                flog "APUB board watchdog service not running on local node"
        	rm $tmp_file
                return $exit_failure
        fi
        flog "APUB board watchdog service running on local node"

        flog "Healthcheck succeeded on local node"

        # Check if -l option is specified
        if [ $a_local == $FALSE ]; then # If no -l option is specified, then check health of remote node

                # Checking status of peer node
                check_peer_node_state
                if [ $? == 0 ]; then
                        # Launch command on remote node
                        ${SSH} -n $remote_node $launchCommand 2>/dev/null > $tmp_file
                        result=`eval $parsePrintout`
                        checkHealth "remote" $FUNCNAME
                        if [ $? == $exit_failure ]; then
                                flog "APUB board watchdog service not running on remote node"
        			rm $tmp_file
                                return $exit_failure
                        fi

                        flog "APUB board watchdog service running on remote node"
                        flog "Healthcheck succeeded on remote node"
                else
                        flog "Remote node is not up. Health not checked on remote node"
                fi
        fi

        rm $tmp_file

	trace_leave $FUNCNAME
	return $exit_success
}

#---------------------------------------------------------
function IO_HCRS_26(){

	trace_enter $FUNCNAME

        tmp_file=`mktemp -t tempfile.XXXXXX`

        launchCommand1="$HWMSCBLS"
        launchCommand2="$HWMXLS"

        parsePrintout="$CAT $tmp_file | $GREP -i -A2 status | $AWK '{print \$(NF-1)}'"
	output="master"

	shelf_info=`check_shelf_info`
	[ $shelf_info == 0 ] && launchCommand=$launchCommand1
	[ $shelf_info == 1 ] && launchCommand=$launchCommand2

        # Checking health on local node
        $launchCommand > $tmp_file
	if [ $? -ne 0 ]; then
		flog "Command launch failed on local node"
        	rm $tmp_file
		return $exit_failure
	fi
        cmdop=`eval $parsePrintout`
        res=`$ECHO $cmdop | $AWK '{if($1=="STATUS") print \$2}'`
        result=`$ECHO $res`
        checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
        	res=`$ECHO $cmdop | $AWK '{if($1=="STATUS") print $3}'`
	        result=`$ECHO $res`
        	checkHealth "local" $FUNCNAME
	        if [ $? == $exit_failure ]; then
                	flog "Check for Master SCB/SCXB failed on local node"
        		rm $tmp_file
	                return $exit_failure
		fi
        fi
        flog "Check for Master SCB-RP/SCXB succeeded on local node"
        flog "Healthcheck succeeded"

        rm $tmp_file

	trace_leave $FUNCNAME
	return $exit_success
}

#---------------------------------------------------------
function IO_HCRS_27(){

	trace_enter $FUNCNAME

        tmp_file=`mktemp -t tempfile.XXXXXX`

        launchCommand="$CMW_STATUS node su comp"

        parsePrintout="$CAT $tmp_file"
        output="Status OK"

        # Checking health on local node
        $launchCommand > $tmp_file
        result=`eval $parsePrintout`
        checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
        	flog "Check to verify that both APUB boards are working failed on local node"
        	rm $tmp_file
                return $exit_failure
        fi
        flog "Check to verify that both APUB boards are working succeeded on local node"
        flog "Healthcheck succeeded"

        rm $tmp_file

	trace_leave $FUNCNAME
	return $exit_success
}

#---------------------------------------------------------
function IO_HCRS_28(){

	trace_enter $FUNCNAME

        tmp_file=`mktemp -t tempfile.XXXXXX`


        launchCommand="$RAIDMGMT"

	# Check the data disk replication type
	DD_REP_TYP=$(immlist -a dataDiskReplicationType axeFunctionsId=1 2>/dev/null | awk -F'=' '{print $2}')

	[ $DD_REP_TYP -eq 2 ] && launchCommand="${RAIDMGR} -s"
	[ $DD_REP_TYP -eq 1 ] && launchCommand="$RAIDMGMT"

	parsePrintout="$CAT $tmp_file | $SED '/^$/d'"
        output1="DOWN"
        output2="UP"
	remote_output=''

	# Checking DRBD status on local node
        $launchCommand > $tmp_file
        result=`eval $parsePrintout`
	output=$output1
	remote_output=$output2
        checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
		output=$output2
		remote_output=$output1
	        checkHealth "local" $FUNCNAME
        	if [ $? == $exit_failure ]; then
                	flog "Check to verify that both AP disk boards are working failed on local node"
        		rm $tmp_file
	                return $exit_failure
		fi
        fi
        flog "Check to verify that both AP disk boards are working succeeded on local node"
        flog "Healthcheck succeeded on local node"

	if [ $a_local == $FALSE ]; then #  If '-l' option is specified, rule should not be executed.
	# Checking status of peer node
        check_peer_node_state
       	if [ $? == 0 ]; then
       		# Launch command on remote node
	        ${SSH} -n $remote_node $launchCommand 2>/dev/null > $tmp_file
       		result=`eval $parsePrintout`
		output=$remote_output
                checkHealth "remote" $FUNCNAME
       	        if [ $? == $exit_failure ]; then
               		flog "Check to verify that both AP disk boards are working failed on remote node"
        		rm $tmp_file
                       	return $exit_failure
		fi
		flog "Check to verify that both AP disk boards are working succeeded on remote node"
       	      	flog "Healthcheck succeeded on remote node"
        else
       		flog "Remote node is not up. Check to verify that both AP disk boards are working failed on remote node"
        	rm $tmp_file
               	return $exit_failure
       	fi
        rm $tmp_file
	fi
}
#---------------------------------------------------------
function IO_HCRS_29(){

	trace_enter $FUNCNAME

#	HW_TYPE=$(GetHwType)
#        if [[ $HW_TYPE == 'VM' ]] ;then
#                flog "Internal VLANs are not configured for vAPG."
#                trace_leave $FUNCNAME
#        	return $exit_success
#        fi

        tmp_file=`mktemp -t tempfile.XXXXXX`

        launchCommand="$CAT $vlan_config"

        parsePrintout="$CAT $tmp_file | $TAIL -n +3 | $WC -l"

        # Checking health on local node
        $launchCommand > $tmp_file
        result=`eval $parsePrintout`
        if [ $result -lt 2 ]; then
                flog "Check to verify APZ VLAN tables failed on local node"
        	rm $tmp_file
                return $exit_failure
        fi
        flog "Check to verify APZ VLAN tables succeeded on local node"
        flog "Healthcheck succeeded"

        rm $tmp_file

	trace_leave $FUNCNAME
	return $exit_success
}

#---------------------------------------------------------
function IO_HCRS_30(){

	trace_enter $FUNCNAME

        tmp_file=`mktemp -t tempfile.XXXXXX`

        launchCommand1="immlist -a apzProtocolType axeFunctionsId=1"
	launchCommand2="$MML saosp | grep 'APZ TYPE'"
	launchCommand3="$MML saosp | grep 'APZ VERSION'"
	launchCommand4="$MML -cp CP1 saosp | find \"APZ TYPE\""
	launchCommand5="$MML -cp CP1 saosp | find \"APZ VERSION\""

        parsePrintout1="$CAT $tmp_file | $CUT -d= -f2"
        parsePrintout2="$CAT $tmp_file | $AWK '{print \$4}'"

        # Checking health on local node
        $launchCommand1 > $tmp_file
        result=`eval $parsePrintout1`
	case $result in
		1)
			output="APZ 212 30"
		;;
		2)
			output="APZ 212 40"
		;;
		3)
			output="APZ 212 50"
		;;
		4)
			output="APZ 212 55"
		;;
		*)
			flog "Test to verify APZ Type mismatch failed"
        		rm $tmp_file
			return $exit_failure
		;;
	esac

	check_cp_state
        if [ $? -ne 0 ]; then
                flog "Failed to retrieve CP information"
        	rm $tmp_file
                return $exit_failure
        fi
	if [ $singleCP == 1 ]; then
		eval $launchCommand2 2>/dev/null > $tmp_file
		type=`eval $parsePrintout2`
		eval $launchCommand3 2>/dev/null > $tmp_file
		version=`eval $parsePrintout2`
		if [[ $type == `echo $output | $AWK '{print $2}'` && $version -ge `echo $output | $AWK '{print $3}'` ]]; then
		        flog "Check to verify APZ Type succeeded on local node for single CP"
		else
		        flog "Check to verify APZ Type failed on local node"
        		rm $tmp_file
			return $exit_failure
		fi
	fi

        if [ $multiCP == 1 ]; then
                eval $launchCommand4 2>/dev/null > $tmp_file
                type=`eval $parsePrintout2`
                eval $launchCommand5 2>/dev/null > $tmp_file
                version=`eval $parsePrintout2`
                if [[ $type == `echo $output | $AWK '{print $2}'` && $version -ge `echo $output | $AWK '{print $3}'` ]]; then
                        flog "Check to verify APZ Type succeeded on local node for multi CP"
                else
                        flog "Check to verify APZ Type failed on local node"
        		rm $tmp_file
                        return $exit_failure
                fi
        fi

        flog "Healthcheck succeeded"
        rm $tmp_file

	trace_leave $FUNCNAME
	return $exit_success
}

#---------------------------------------------------------
function IO_HCRS_31(){
	trace_enter $FUNCNAME
	local node_id=$(< /etc/cluster/nodes/this/id)
	local cmd_hwtype='/opt/ap/apos/conf/apos_hwtype.sh'
	local HW_TYPE=$( $cmd_hwtype)
	service_opt="\
        apg-dhcpd.service"
	[ $HW_TYPE == "VM" ] && {
		DHCP_STATE=$(${IMMLIST} -a value axeInfoId=apg_dhcp | cut -d "=" -f2)
		[ $DHCP_STATE == "OFF" ] && service_opt=""
		}
	# List of basic services to be monitored on the ACTIVE AP1 node
	local services_active_ap1="\
		apg-rsh.socket \
		apg-vsftpd.socket \
		apg-vsftpd-APIO_1.socket \
		apg-vsftpd-APIO_2.socket \
		apg-vsftpd-nbi.socket \
		apg-atftpd@192.168.169.$node_id.service \
		apg-atftpd@192.168.170.$node_id.service \
		apg-atftpd@192.168.169.33.service \
		apg-atftpd@192.168.170.33.service \
		rsyslog.service \
		lde-failoverd.service \
		auditd.service $service_opt"
	
	# List of basic services to be monitored on the PASSIVE AP1 node
	local services_passive_ap1="\
		apg-rsh.socket \
		rsyslog.service \
		lde-failoverd.service \
		auditd.service"

	# List of basic services to be monitored on the ACTIVE AP2 node
	local services_active_ap2="\
		apg-rsh.socket \
		apg-vsftpd.socket \
		apg-vsftpd-nbi.socket \
		rsyslog.service \
		lde-failoverd.service \
		auditd.service"
	
	# List of basic services to be monitored on the PASSIVE AP2 node
	local services_passive_ap2="\
		apg-rsh.socket \
		rsyslog.service \
		lde-failoverd.service \
		auditd.service"
	
	launchCommand2="$PS aux | $GREP com"
	launchCommand3="$CMW_STATUS node su comp"
		
	local AP=$(getAPType)
	if [ $AP -eq 1 ]; then 
		services_active=$services_active_ap1
		services_passive=$services_passive_ap1
	else
		services_active=$services_active_ap2
		services_passive=$services_passive_ap2
	fi
						
	# Checking health on local node
	# Check the status of service on active node
	for hc_service in $services_active
	do
		# Check the service status: the script returns:
		# - 0: if the service is running
		# - 1: if the service is not running
		${SERVICEMGMT} status $hc_service &> /dev/null 
		if [ $? -eq 1 ]; then
			flog "Service $hc_service not running on local node"
			return $exit_failure
		fi 
	done
	flog "All services up and running on local node"
	
	tmp_file=$(mktemp -t tempfile.XXXXXX)
	eval $launchCommand2 > $tmp_file
	result=$($CAT $tmp_file | $GREP -c /opt/com/bin/com)
	if [ $result -lt 1 ]; then
		flog "COM service not running on local node"
		rm $tmp_file
		return $exit_failure
	fi
	flog "COM service check succeeded on local node"

	eval $launchCommand3 > $tmp_file
	result=$?
	output=0
	checkHealth "local" $FUNCNAME
	if [ $? == $exit_failure ]; then
		flog "Check for opensafd failed on local node"
		rm $tmp_file
		return $exit_failure
	fi
	flog "Opensafd status succeeded on local node"	
	flog "Healthcheck succeeded on local node"

	# Check if -l option is specified
	if [ $a_local == $FALSE ]; then # If no -l option is specified, then check health of remote node
		# Checking status of peer node
		check_peer_node_state
		if [ $? == 0 ]; then
			# Launch command on remote node
			# Check the status of service on passive node
			for hc_service in $services_passive
			do
				# Check the service status: the script returns:
				# - 0: if the service is running
				# - 1: if the service is not running
				${SSH} -n $remote_node ${SERVICEMGMT} status $hc_service &> /dev/null
				if [ $? -eq 1 ]; then
					flog "Service $hc_service not running on remote node"
					rm $tmp_file					
					return $exit_failure
				fi 
			done
			flog "All services up and running on remote node"
	
			${SSH} -n $remote_node $launchCommand2 2>/dev/null > $tmp_file
			result=$($CAT $tmp_file | $GREP -c /opt/com/bin/com)
			if [ $result -lt 1 ]; then
				flog "COM service not running on remote node"
				rm $tmp_file
				return $exit_failure
			fi
			flog "COM service check succeeded on remote node"

			${SSH} -n $remote_node $launchCommand3 2>/dev/null > $tmp_file
			result=$?
			output=0
			checkHealth "remote" $FUNCNAME
			if [ $? == $exit_failure ]; then
				flog "Check for opensafd failed on remote node"
				rm $tmp_file
				return $exit_failure
			fi
			flog "Opensafd status succeeded on remote node"
			flog "Healthcheck succeeded on remote node"
		else
			flog "Remote node is not up. Check to verify basic services failed on remote node"
			rm $tmp_file
			return $exit_failure
		fi
	fi
	rm $tmp_file

	trace_leave $FUNCNAME
	return $exit_success
}

#---------------------------------------------------------
function IO_HCRS_32(){
	trace_enter $FUNCNAME

	tmp_file=$(mktemp -t tempfile.XXXXXX)
	tmp_file_ssh=$(mktemp -t tempfile.XXXXXX)

	# The following parameters are being verified to check the health of SSH configuration. If any parameters in SSH are modified, this function is bound to change.

	# File /etc/ssh/ssh_config
	output_ssh1_ap1="Host SC-2-1 sc-2-1 169.254.208.1 SC-2-2 sc-2-2 169.254.208.2"
	output_ssh1_ap2="Host SC-2-1 sc-2-1 169.254.208.3 SC-2-2 sc-2-2 169.254.208.4"
	output_ssh2="HostbasedAuthentication yes"
	output_ssh3="Port 1022"

	# File /etc/ssh/sshd_config_22
	op_sshd_22_1="Port 22"
	op_sshd_22_2="PasswordAuthentication yes"
	op_sshd_22_3="UsePAM yes"
	op_sshd_22_4="UsePrivilegeSeparation no"
	op_sshd_22_5="Subsystem sftp internal-sftp"
	op_sshd_22_6="Subsystem cli /opt/ap/apos/conf/apos_subsystem_wrapper.sh"
	op_sshd_22_7="PrintLastLog no"
	#op_sshd9="Port 4422"
	op_sshd_22_8="ChrootDirectory /data/opt/ap/nbi_fuse"

	current_node_internal_ip=`$CAT $internal_ip_file`
	
	# File /etc/ssh/sshd_config_internal
	op_int1="PasswordAuthentication no"
	op_int2="UsePAM yes"
	op_int3="Subsystem       sftp    /usr/lib/ssh/sftp-server"
	op_int4="HostbasedAuthentication yes"
	op_int5="IgnoreRhosts no"
	op_int6="Port 1022"
	op_int7="ListenAddress $current_node_internal_ip"

	# File /etc/ssh/sshd_config_830
	op_sshd_830_1="Port 830"
	op_sshd_830_2="PasswordAuthentication yes"
	op_sshd_830_3="UsePAM yes"
	op_sshd_830_4="UsePrivilegeSeparation no"
	op_sshd_830_5="Subsystem netconf /opt/com/bin/netconf"
	op_sshd_830_6="PrintLastLog no"

	launchCommand1="$CLUSTER config -v"
	launchCommand2="$CAT $cliss_subshell_file | $GREP \"<executable>\""
	launchCommand3="$CAT $ssh_config"
	launchCommand4="$CAT $sshd_config_22"
	launchCommand5="$CAT $sshd_internal"
	launchCommand6="$CAT $sshd_config_830"	

	output1=0

	# Checking health on local node
	# Checking SSH configuration for ssh_config

	local AP=$(getAPType)
	if [ $AP -eq 1 ]; then
		output_ssh1=$output_ssh1_ap1
	else
		output_ssh1=$output_ssh1_ap2
	fi
		
	$launchCommand3 > $tmp_file
	$ECHO -e "$output_ssh1 \n$output_ssh2 \n$output_ssh3" > $tmp_file_ssh
	for line in $($CAT $tmp_file_ssh)
	do
		$CAT $tmp_file | $GREP -wq $line
		if [ $? -ne 0 ]; then
			flog "Configuration check for ssh_config failed on local node"
			rm $tmp_file
			rm $tmp_file_ssh
			return $exit_failure
		fi
	done
	flog "Configuration check for ssh_config succeeded on local node"

	# Checking SSH configuration for sshd_config_22
	$launchCommand4 > $tmp_file
	$ECHO -e "$op_sshd_22_1 \n$op_sshd_22_2 \n$op_sshd_22_3 \n$op_sshd_22_4 \n$op_sshd_22_5 \n$op_sshd_22_6 \n$op_sshd_22_7 \n$op_sshd_22_8" > $tmp_file_ssh
	while read line
	do
		$CAT $tmp_file | sed -e 's/^[ \t]*//' | $GREP -wq "^$line"
		if [ $? -ne 0 ]; then
			flog "Configuration check for sshd_config_22 failed on local node"
			rm $tmp_file
			rm $tmp_file_ssh
			return $exit_failure
		fi
	done < $tmp_file_ssh
	flog "Configuration check for sshd_config_22 succeeded on local node"

	# Checking SSH configuration for sshd_config_830
	$launchCommand6 > $tmp_file
	$ECHO -e "$op_sshd_830_1 \n$op_sshd_830_2 \n$op_sshd_830_3 \n$op_sshd_830_4 \n$op_sshd_830_5 \n$op_sshd_830_6" > $tmp_file_ssh
	while read line
	do
	$CAT $tmp_file | sed -e 's/^[ \t]*//' | $GREP -wq "^$line"
		if [ $? -ne 0 ]; then
			flog "Configuration check for sshd_config_830 failed on local node"
			rm $tmp_file
			rm $tmp_file_ssh
			return $exit_failure
		fi
	done < $tmp_file_ssh
	flog "Configuration check for sshd_config_830 succeeded on local node"

	# Checking SSH configuration for sshd_config_internal
	$launchCommand5 > $tmp_file
	$ECHO -e "$op_int1 \n$op_int2 \n$op_int3 \n$op_int4 \n$op_int5 \n$op_int6 \n$op_int7" > $tmp_file_ssh
	for line in $($CAT $tmp_file_ssh)
	do
		$CAT $tmp_file | $GREP -wq $line
		if [ $? -ne 0 ]; then
			flog "Configuration check for sshd_config_internal failed on local node"
			rm $tmp_file
			rm $tmp_file_ssh
			return $exit_failure
		fi
	done
	flog "Configuration check for sshd_config_internal succeeded on local node"

	# Checking cluster configuration
	$launchCommand1 &> $tmp_file
	result=$?
	output=$output1
	checkHealth "local" $FUNCNAME
	if [ $? == $exit_failure ]; then
		flog "Cluster configuration check failed on local node"
		rm $tmp_file
		rm $tmp_file_ssh
		return $exit_failure
	fi
	flog "Cluster configuration check succeeded on local node"

	# Checking that all the subshelled commands are mapped to real files on the file system
	flog "Checking that all the subshelled commands are mapped to real files on the file system on local node"
	eval $launchCommand2 > $tmp_file
	if [ $? -ne 0 ]; then
		flog "File for COM subshell commands does not exist"
		rm $tmp_file
		rm $tmp_file_ssh
		return $exit_failure
	fi

	for file in $($CAT $tmp_file | $SED 's/[[:space:]]*<executable>//g' | $CUT -d"<" -f1)
	do
		if [ -e $file ]; then
			if [ -L $file ]; then
				orig_file=$($LS -l $file | $AWK '{print $NF}')
				if [ ! -e $orig_file ]; then
					flog "No soft link file existing for $file on local node"
					rm $tmp_file
					rm $tmp_file_ssh
					return $exit_failure
				fi
			fi
		else
			flog "No real file existing for $file on local node"
			rm $tmp_file
			rm $tmp_file_ssh
			return $exit_failure
		fi
	done
	flog "CLISS configuration check succeeded on local node"
	flog "Healthcheck succeeded on local node"

#####################################################
        # Check if -l option is specified
        if [ $a_local == $FALSE ]; then # If no -l option is specified, then check health of remote node

	        # Checking status of peer node
        	check_peer_node_state
	        if [ $? == 0 ]; then
        	        # Launch command on remote node
        		# Checking SSH configuration for ssh_config
			${SSH} -n $remote_node $launchCommand3 2>/dev/null > $tmp_file
		        $ECHO -e "$output_ssh1 \n$output_ssh2 \n$output_ssh3" > $tmp_file_ssh
		        for line in $($CAT $tmp_file_ssh)
		        do
		                $CAT $tmp_file | $GREP -wq $line
                		if [ $? -ne 0 ]; then
		                        flog "Configuration check for ssh_config failed on remote node"
        				rm $tmp_file
                		        return $exit_failure
		                fi
		        done
		        flog "Configuration check for ssh_config succeeded on remote node"

			# Checking SSH configuration for sshd_config_22
			${SSH} -n $remote_node $launchCommand4 2>/dev/null > $tmp_file
			$ECHO -e "$op_sshd_22_1 \n$op_sshd_22_2 \n$op_sshd_22_3 \n$op_sshd_22_4 \n$op_sshd_22_5 \n$op_sshd_22_6 \n$op_sshd_22_7 \n$op_sshd_22_8" > $tmp_file_ssh
			while read line
			do
				$CAT $tmp_file | sed -e 's/^[ \t]*//' | $GREP -wq "^$line"
				if [ $? -ne 0 ]; then
					flog "Configuration check for sshd_config_22 failed on remote node"
					rm $tmp_file
					return $exit_failure
				fi
			done < $tmp_file_ssh
			flog "Configuration check for sshd_config_22 succeeded on remote node"

			# Checking SSH configuration for sshd_config_830
			${SSH} -n $remote_node $launchCommand6 2>/dev/null > $tmp_file
			$ECHO -e "$op_sshd_830_1 \n$op_sshd_830_2 \n$op_sshd_830_3 \n$op_sshd_830_4 \n$op_sshd_830_5 \n$op_sshd_830_6" > $tmp_file_ssh
			while read line
			do
				$CAT $tmp_file | sed -e 's/^[ \t]*//' | $GREP -wq "^$line"
				if [ $? -ne 0 ]; then
					flog "Configuration check for sshd_config_830 failed on remote node"
					rm $tmp_file
					return $exit_failure
				fi
			done  < $tmp_file_ssh
			flog "Configuration check for sshd_config_830 succeeded on remote node"

			# Checking SSH configuration for sshd_config_internal
			local peer_node_internal_ip_file="$CAT $internal_ip_file"
			${SSH} -n $remote_node $launchCommand5 2>/dev/null > $tmp_file
			local peer_node_internal_ip=$(${SSH} -n $remote_node $peer_node_internal_ip_file 2>/dev/null)
			if [ -z "$peer_node_internal_ip" ]; then
				flog "Configuration check for sshd_config_internal failed on remote node"
				rm $tmp_file
				return $exit_failure
			fi
			op_int7="ListenAddress $peer_node_internal_ip"
			$ECHO -e "$op_int1 \n$op_int2 \n$op_int3 \n$op_int4 \n$op_int5 \n$op_int6 \n$op_int7" > $tmp_file_ssh
			for line in $($CAT $tmp_file_ssh)
			do
				$CAT $tmp_file | $GREP -wq $line
				if [ $? -ne 0 ]; then
					flog "Configuration check for sshd_config_internal failed on remote node"
					rm $tmp_file
					return $exit_failure
				fi
			done
			flog "Configuration check for sshd_config_internal succeeded on remote node"
	
		        # Checking that all the subshelled commands are mapped to real files on the file system
        		flog "Checking that all the subshelled commands are mapped to real files on the file system on remote node.."
	                ${SSH} -n $remote_node $launchCommand2 2>/dev/null > $tmp_file
        		if [ $? -ne 0 ]; then
                		flog "File for COM subshell commands does not exist"
        			rm $tmp_file
	                	return $exit_failure
	        	fi
	
		        for file in $($CAT $tmp_file | $SED 's/[[:space:]]*<executable>//g' | $CUT -d"<" -f1)
        		do
                		if [ -e $file ]; then
                        		if [ -L $file ]; then
	                                	orig_file=`$LS -l $file | $AWK '{print $NF}'`
	                                	if [ ! -e $orig_file ]; then
        		                                flog "No soft link file existing for $file on remote node"
        						rm $tmp_file
                		                        return $exit_failure
                        		        fi
	                        	fi
	        	        else
        	        	        flog "No real file existing for $file on remote node"
        				rm $tmp_file
                	        	return $exit_failure
	                	fi
	        	done

		        flog "CLISS configuration check succeeded on remote node"

                	flog "Healthcheck succeeded on remote node"
	        else
        	        flog "Remote node is not up. Check to verify that both AP disk boards are working failed on remote node"
        		rm $tmp_file
                	return $exit_failure
	        fi
	fi

        rm $tmp_file_ssh
        rm $tmp_file

	trace_leave $FUNCNAME
	return $exit_success
}

#---------------------------------------------------------
function IO_HCRS_33(){

	trace_enter $FUNCNAME

        tmp_file=`mktemp -t tempfile.XXXXXX`

        launchCommand="$INTEGRITYAP"

        parsePrintout="$CAT $tmp_file"
        output1="WARNING"
        output2="ERROR"
        remote_output=

        # Checking health on local node
        $launchCommand > $tmp_file
	if [ $? == 3 ]; then
                flog "Baseline file not found. Execute acs_lct_baseline_create to create baseline file."
                return $exit_failure
        fi

	result1=`$CAT $tmp_file | $GREP -i "$output1"`
	result2=`$CAT $tmp_file | $GREP -i "$output2"`
	if [[ ! -z $result1 || ! -z $result2 ]]; then
	       	flog "Check to verify file system integrity failed on local node due to the following reasons:"
               	flog "$result1"
        	flog "$result2"
        	rm $tmp_file
                return $exit_failure
	fi
        flog "Check to verify file system integrity succeeded on local node"
        flog "Healthcheck succeeded on local node"

        # Check if -l option is specified
        if [ $a_local == $FALSE ]; then # If no -l option is specified, then check health of remote node

	        # Checking status of peer node
        	check_peer_node_state
	        if [ $? == 0 ]; then
        	        # Launch command on remote node
                	${SSH} -n $remote_node $launchCommand 2>/dev/null > $tmp_file
			result1=`$CAT $tmp_file | $GREP -i "$output1"`
			result2=`$CAT $tmp_file | $GREP -i "$output2"`
			if [[ ! -z $result1 || ! -z $result2 ]]; then
        		        flog "Check to verify file system integrity failed on remote node due to the following reasons:"
        		        flog "$result1"
        		        flog "$result2"
        			rm $tmp_file
                		return $exit_failure
		        fi
        		flog "Check to verify file system integrity succeeded on remote node"
	        	flog "Healthcheck succeeded on remote node"
	        else
        	        flog "Remote node is not up. Check to verify file system integrity failed on remote node"
        		rm $tmp_file
                	return $exit_failure
        	fi
	fi

        rm $tmp_file

	trace_leave $FUNCNAME
	return $exit_success
}

#---------------------------------------------------------
function IO_HCRS_34(){

	trace_enter $FUNCNAME

        tmp_file=`mktemp -t tempfile.XXXXXX`

        launchCommand="$IPMITOOL rs"
        launchCommand1="$IPMITOOL rgpr ram 0"
        launchCommand2="$IPMITOOL rgpr ram 1"
        launchCommand3="$IPMITOOL rgpr nvram 0"
        launchCommand4="$IPMITOOL dfx"
        launchCommand5="$IPMITOOL gif"
        launchCommand6="$IPMITOOL rbiosp"
        launchCommand7="$IPMITOOL rbcsgep4"
        launchCommand8="$IPMITOOL rbcsgep5"
        launchCommand9="$IPMITOOL rbcsgep7"

        output1="01"
	output_temp="c4"
	output_ram0="01 00 00 00"
	output_ram1="00 00 00 00"
        output_nvram0_gep2_2="07 02 00 00"
        output_nvram0_gep2_4="07 04 00 00"
        output_nvram0_gep2_6="07 06 00 00"
	output_nvram0_gep1="07 00 00 00"
	output_ipmifw="(UPG)"
	output_bios=1
        output="10 01 ff ff ff ff ff ff ff ff ff ff ff ff ff ff 04 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00"
	
	parsePrintout1="$CAT $tmp_file | $TAIL -1 | $SED 's/^[[:space:]]//g'"
	parsePrintout2="$CAT $tmp_file | $HEAD -3 | $GREP ^0001 | $AWK '{print \$3,\$4}'"
	parsePrintout3="$CAT $tmp_file | $GREP -i running | $AWK '{print \$3}'"
	parsePrintout4="$CAT $tmp_file | $TAIL -1 | $CUT -d: -f 2 | $SED 's/^[[:space:]]//g'"
        parsePrintout5="$CAT $tmp_file | $HEAD -3 | $TAIL -1 "

        # Checking health on local node
	# Checking health of sensor table / voltage sensors
        $launchCommand > $tmp_file
	if [ $? -ne 0 ]; then
		flog "Command launch failed on local node"
        	rm $tmp_file
		return $exit_failure
	fi
	output=$output1
	sensor_result=0
	old_IFS=$IFS
	IFS=$'\n'
        for i in $($CAT $tmp_file | $TAIL -n +5)
        do
                output="01"
                sensor_nr=`echo $i | $AWK '{print $1,$2}'`
                [ "$sensor_nr" == "Sensor 128" ] && continue
                result=`echo $i | $AWK '{print $(NF-1)}'`
                checkHealth "local" $FUNCNAME
                if [ $? == $exit_failure ]; then
                        if [ "$sensor_nr" == "Sensor 131" ]; then
                                output="40"
                                if [ $output -ne $result ]; then
                                        flog "$sensor_nr is reporting an alarm on local node"
                                        sensor_result=1
                                fi
                        elif [[ "$sensor_nr" == "Sensor 41" || "$sensor_nr" == "Sensor 42" ]]; then
                        	sensor_result=0
			#        sensor_value=`echo $i | $AWK -F- '{print \$2}' | $SED 's/^  //g'`
                        #        if [ "$sensor_value" == "00 00 00 00 c0 40 00" ]; then
                        #                flog "WARNING: $sensor_nr is reporting an alarm on local node"
                        #        else
                        #                flog "$sensor_nr is reporting an alarm on local node"
                        #                sensor_result=1
                        #        fi
                        else
                        	flog "$sensor_nr is reporting an alarm on local node"
                        	sensor_result=1
                        fi
                fi
        done
        IFS=$old_IFS
        if [ $sensor_result == 0 ]; then
                flog "No serious fault in sensor table detected on local node"
        else
                flog "Check for sensor table failed on local node"
                return $exit_failure
        fi

        # Checking health of temperature sensors
        output=$output_temp
        old_IFS=$IFS
        IFS=$'\n'
        for i in $($CAT $tmp_file | $HEAD -4 | $TAIL -3)
        do
                result=`echo $i | $AWK '{print $(NF-1)}'`
                checkHealth "local" $FUNCNAME
                if [ $? == $exit_failure ]; then
                	flog "Check for aggregated temperature failed on local node"
        		rm $tmp_file
	                return $exit_failure
                fi
        done
        IFS=$old_IFS

        gep_type=`check_gep_info`

        if [ $gep_type == 4 ]; then

        #Checking health of BCS Registers
        eval $launchCommand7 > $tmp_file
        if [ $? -ne 0 ]; then
                flog "Command launch failed on local node"
                rm $tmp_file
                return $exit_failure
        fi
        result=`eval $parsePrintout5`
        output=$output
        checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
                flog "BCS status for gep 4 reported error on local node"
                rm $tmp_file
                return $exit_failure
        fi
        flog "BCS status for gep 4 succeeded on local node"

        elif [ $gep_type == 5 ]; then

        #Checking health of BCS Registers
        eval $launchCommand8 > $tmp_file
        if [ $? -ne 0 ]; then
                flog "Command launch failed on local node"
                rm $tmp_file
                return $exit_failure
        fi
        result=`eval $parsePrintout5`
        output=$output
        checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
                flog "BCS status for gep 5 reported error on local node"
                rm $tmp_file
                return $exit_failure
        fi
        flog "BCS status for gep 5 succeeded on local node"
         
        elif [ $gep_type == 7 ]; then

        #Checking health of BCS Registers
        eval $launchCommand9 > $tmp_file
        if [ $? -ne 0 ]; then
                flog "Command launch failed on local node"
                rm $tmp_file
                return $exit_failure
        fi
        result=`eval $parsePrintout5`
        output=$output
        checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
                flog "BCS status for gep 7 reported error on local node"
                rm $tmp_file
                return $exit_failure
        fi
        flog "BCS status for gep 7 succeeded on local node"

        else


	# Checking health of GPR registers
        eval $launchCommand1 > $tmp_file
	if [ $? -ne 0 ]; then
		flog "Command launch failed on local node"
        	rm $tmp_file
		return $exit_failure
	fi
	result=`eval $parsePrintout1`
	output=$output_ram0
	checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
        	flog "GPR status for ram 0 reported error on local node"
        	rm $tmp_file
                return $exit_failure
	fi
        flog "GPR status for ram 0 succeeded on local node"

        eval $launchCommand2 > $tmp_file
	if [ $? -ne 0 ]; then
		flog "Command launch failed on local node"
        	rm $tmp_file
		return $exit_failure
	fi
	result=`eval $parsePrintout1`
	output=$output_ram1
	checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
        	flog "GPR status for ram 1 reported error on local node"
        	rm $tmp_file
                return $exit_failure
	fi
        flog "GPR status for ram 1 succeeded on local node"
        fi
        eval $launchCommand3 > $tmp_file
	if [ $? -ne 0 ]; then
		flog "Command launch failed on local node"
        	rm $tmp_file
		return $exit_failure
	fi
        #Check whether the node is GEP1 or GEP2
        gep_type=`check_gep_info`

        if [ $gep_type == 1 ]; then
                output_nvram0=$output_nvram0_gep1
        elif [ $gep_type == 2 ]; then
                # Check for baud rate
                # if baud=38400, nvram 0=07 02 00 00
                # if baud=57600, nvram 0=07 02 00 00
                # if baud=115200, nvram 0=07 02 00 00
                local baud_rate=$($CAT /proc/cmdline | $CUT -d, -f2 | $AWK '{print $1}')
                [ $baud_rate == 38400 ] && output_nvram0=$output_nvram0_gep2_2
                [ $baud_rate == 57600 ] && output_nvram0=$output_nvram0_gep2_4
                [ $baud_rate == 115200 ] && output_nvram0=$output_nvram0_gep2_6
        fi

	result=`eval $parsePrintout1`
	output=$output_nvram0

	checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
        	flog "GPR status for nvram 0 reported warning on local node"
        	flog "GPR status for nvram 0 is expected to be $output_nvram0"
        	#rm $tmp_file
                #return $exit_failure
	fi
        flog "GPR status for nvram 0 succeeded on local node"

	# Checking Fatal Event Log
        eval $launchCommand4 > $tmp_file
        if [ $? -ne 0 ]; then
                flog "Command launch failed on local node"
        	rm $tmp_file
                return $exit_failure
        fi
        result=`eval $parsePrintout2`
	fel_date=`$DATE -d "$result" +%s`
	curr_date=`$DATE +%s`
        if [ $(( $curr_date - $fel_date )) -lt 86400 ]; then
                flog "FATAL EVENT LOG has an entry in the last 24 hours on local node"
        	rm $tmp_file
                return $exit_failure
        fi
        flog "FEL is clear in the last 24 hours on local node"

	# Check if IPMI FW is running 
        eval $launchCommand5 > $tmp_file
        if [ $? -ne 0 ]; then
                flog "Command launch failed on local node"
        	rm $tmp_file
                return $exit_failure
        fi
        result=`eval $parsePrintout3`
        output=$output_ipmifw
        checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
                flog "Checking of IPMI FW failed on local node"
		if [ $result == "(FB)" ]; then
                	flog "WARNING: FB is running as IPMI FW"
		else
        		rm $tmp_file
                	return $exit_failure
		fi
        fi
        flog "Checking of IPMI FW succeeded on local node"

        # Check if BIOS FW is running
        eval $launchCommand6 > $tmp_file
        if [ $? -ne 0 ]; then
                flog "Command launch failed on local node"
        	rm $tmp_file
                return $exit_failure
        fi
        result=`eval $parsePrintout4`
        output=$output_bios
        checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
                flog "Checking of IPMI BIOS failed on local node"
                if [ $result == "0" ]; then
                        flog "WARNING: FB is running in IPMI BIOS"
                else
        		rm $tmp_file
                        return $exit_failure
                fi
        fi
        flog "Checking of IPMI BIOS succeeded on local node"

        flog "Check to verify IPMI checks succeeded on local node"
        flog "Healthcheck succeeded on local node"

        # Check if -l option is specified
        if [ $a_local == $FALSE ]; then # If no -l option is specified, then check health of remote node

	        # Checking status of peer node
        	check_peer_node_state
	        if [ $? == 0 ]; then
        	        # Launch command on remote node
	        	# Checking health of sensor table
	                ${SSH} -n $remote_node $launchCommand 2>/dev/null > $tmp_file
		        if [ $? -ne 0 ]; then
        		        flog "Command launch failed on remote node"
        			rm $tmp_file
                		return $exit_failure
		        fi
        		output=$output1
	        	sensor_result=0
	        	old_IFS=$IFS
		        IFS=$'\n'
        		for i in $($CAT $tmp_file | $TAIL -n +5)
		        do
        		        sensor_nr=`echo $i | $AWK '{print $1,$2}'`
                		result=`echo $i | $AWK '{print $(NF-1)}'`
	                	checkHealth "remote" $FUNCNAME
	        	        if [ $? == $exit_failure ]; then
        	        	        flog "$sensor_nr is reporting an alarm on remote node"
                	        	sensor_result=1
	                	fi
	        	done
		        IFS=$old_IFS
        		if [ $sensor_result ]; then
                		flog "No fault in sensor table detected on remote node"
		        else
        		        flog "Check for sensor table failed on remote node"
                		#return $exit_failure
		        fi

		        output=$output_temp
		        old_IFS=$IFS
		        IFS=$'\n'
		        for i in $($CAT $tmp_file | $HEAD -4 | $TAIL -3)
		        do
                		result=`echo $i | $AWK '{print $(NF-1)}'`
		                checkHealth "remote" $FUNCNAME
                		if [ $? == $exit_failure ]; then
		                        flog "Check for aggregated temperature failed on remote node"
        				rm $tmp_file
                		        return $exit_failure
		                fi
		        done
		        IFS=$old_IFS

                        gep_type=`check_gep_info`
                        if [ $gep_type == 4 ]; then

                                  #Checking health of BCS Registers
                                  ${SSH} -n $remote_node $launchCommand7 2>/dev/null > $tmp_file
                                 if [ $? -ne 0 ]; then
                                         flog "Command launch failed on remote node"
                                                 rm $tmp_file
                                                 return $exit_failure
                                 fi
                                result=`eval $parsePrintout5`
                                 output=$output
                                 checkHealth "remote" $FUNCNAME
                                 if [ $? == $exit_failure ]; then
                                         flog "BCS status for gep 4 reported error on remote node"
                                         rm $tmp_file
                                         return $exit_failure
                                 fi
                                 flog "BCS status for gep 4 succeeded on remote node"
                        elif [ $gep_type == 5 ]; then

                                  #Checking health of BCS Registers
                                  ${SSH} -n $remote_node $launchCommand8 2>/dev/null > $tmp_file
                                 if [ $? -ne 0 ]; then
                                         flog "Command launch failed on remote node"
                                                 rm $tmp_file
                                                 return $exit_failure
                                 fi
                                result=`eval $parsePrintout5`
                                 output=$output
                                 checkHealth "remote" $FUNCNAME
                                 if [ $? == $exit_failure ]; then
                                         flog "BCS status for gep 5 reported error on remote node"
                                         rm $tmp_file
                                         return $exit_failure
                                 fi
                                 flog "BCS status for gep 5 succeeded on remote node"
        		elif [ $gep_type == 7 ]; then

                                #Checking health of BCS Registers
                                ${SSH} -n $remote_node $launchCommand9 2>/dev/null > $tmp_file
                                if [ $? -ne 0 ]; then
                                         flog "Command launch failed on remote node"
                                                 rm $tmp_file
                                                 return $exit_failure
                                fi
                                result=`eval $parsePrintout5`
                                 output=$output
                                 checkHealth "remote" $FUNCNAME
                                 if [ $? == $exit_failure ]; then
                                         flog "BCS status for gep 7 reported error on remote node"
                                         rm $tmp_file
                                         return $exit_failure
                                 fi
        			 flog "BCS status for gep 7 succeeded on local node"

				 else

		        # Checking health of GPR registers
                	${SSH} -n $remote_node $launchCommand1 2>/dev/null > $tmp_file
		        if [ $? -ne 0 ]; then
        		        flog "Command launch failed on remote node"
        			rm $tmp_file
                		return $exit_failure
		        fi
        		result=`eval $parsePrintout1`
	        	output=$output_ram0
	        	checkHealth "remote" $FUNCNAME
		        if [ $? == $exit_failure ]; then
        		        flog "GPR status for ram 0 reported error on remote node"
        			rm $tmp_file
	                	return $exit_failure
		        fi
        		flog "GPR status for ram 0 succeeded on remote node"

	                ${SSH} -n $remote_node $launchCommand2 2>/dev/null > $tmp_file
        		if [ $? -ne 0 ]; then
                		flog "Command launch failed on remote node"
        			rm $tmp_file
	                	return $exit_failure
	        	fi
		        result=`eval $parsePrintout1`
        		output=$output_ram1
		        checkHealth "remote" $FUNCNAME
        		if [ $? == $exit_failure ]; then
                		flog "GPR status for ram 1 reported error on remote node"
        			rm $tmp_file
	                	return $exit_failure
	        	fi
		        flog "GPR status for ram 1 succeeded on remote node"
                        fi
	
                	${SSH} -n $remote_node $launchCommand3 2>/dev/null > $tmp_file
		        if [ $? -ne 0 ]; then
        		        flog "Command launch failed on remote node"
        			rm $tmp_file
                		return $exit_failure
		        fi
        		result=`eval $parsePrintout1`
	        	output=$output_nvram0
	        	checkHealth "remote" $FUNCNAME
		        if [ $? == $exit_failure ]; then
        		        flog "GPR status for nvram 0 reported warning on remote node"
        			flog "GPR status for nvram 0 is expected to be $output_nvram0"
        			#rm $tmp_file
                		#return $exit_failure
		        fi
        		flog "GPR status for nvram 0 succeeded on remote node"
	
		        # Checking Fatal Event Log
                	${SSH} -n $remote_node $launchCommand4 2>/dev/null > $tmp_file
		        if [ $? -ne 0 ]; then
        		        flog "Command launch failed on remote node"
        			rm $tmp_file
                		return $exit_failure
		        fi	
        		result=`eval $parsePrintout2`
	        	fel_date=`$DATE -d "$result" +%s`
	        	curr_date=`$DATE +%s`
		        if [ $(( $curr_date - $fel_date )) -lt 86400 ]; then
        		        flog "FATAL EVENT LOG has an entry in the last 24 hours on remote node"
        			rm $tmp_file
                		return $exit_failure
		        fi	
        		flog "FEL is clear in the last 24 hours on remote node"
	
        	        ${SSH} -n $remote_node $launchCommand5 2>/dev/null > $tmp_file
	        	if [ $? -ne 0 ]; then
        	        	flog "Command launch failed on remote node"
        			rm $tmp_file
	        	        return $exit_failure
	        	fi
		        result=`eval $parsePrintout3`
        		output=$output_ipmifw
		        checkHealth "remote" $FUNCNAME
        		if [ $? == $exit_failure ]; then
                		flog "Checking of IPMI FW failed on remote node"
		               	if [ $result == "(FB)" ]; then
        		                flog "WARNING: FB is running as IPMI FW"
	        	        else
        				rm $tmp_file
        	        	        return $exit_failure
	                	fi
		        fi

	        	# Check if BIOS FW is running
        	        ${SSH} -n $remote_node $launchCommand6 2>/dev/null > $tmp_file
	        	if [ $? -ne 0 ]; then
        	        	flog "Command launch failed on local node"
        			rm $tmp_file
                		return $exit_failure
		        fi
		        result=`eval $parsePrintout4`
		        output=$output_bios
		        checkHealth "remote" $FUNCNAME
		        if [ $? == $exit_failure ]; then
		                flog "Checking of IPMI BIOS failed on remote node"
		                if [ $result == "0" ]; then
                	        flog "WARNING: FB is running in IPMI BIOS"
		                else
        			rm $tmp_file
        	                return $exit_failure
                		fi
        		flog "Checking of IPMI BIOS succeeded on remote node"
			fi
                	flog "Check to verify IPMI checks succeeded on remote node"
	                flog "Healthcheck succeeded on remote node"
        	else
                	flog "Remote node is not up. Check to verify that both AP disk boards are working failed on remote node"
        		rm $tmp_file
	                return $exit_failure
        	fi
	fi

        rm $tmp_file

	trace_leave $FUNCNAME
	return $exit_success
}

#--------------------------------------------------------
function IO_HCRS_35(){

        trace_enter $FUNCNAME

        tmp_file=`mktemp -t tempfile.XXXXXX`

        DN_name=`$IMMFIND | $GREP -i managedelement | $GREP -v ','`
        if [ -z $DN_name ]; then
                flog "$DN_name DN name for ManagedElement not found"
                rm $tmp_file
                return $exit_failure
        fi

        launchCommand="$IMMLIST $DN_name"

        parsePrintout="$CAT $tmp_file | $GREP networkManagedElementId | $AWK '{print \$3}'"
        #flog "$parsePrintout"
        output="<Empty>"
        # Checking health on local node
        $launchCommand 2>/dev/null > $tmp_file
        if [ $? -ne 0 ]; then
                flog "Command launch failed"
                rm $tmp_file
                return $exit_failure
        fi
        result=`eval $parsePrintout`
        flog "Node name is $result"
        if [ $result == $output ]; then
                flog "Check to verify node name failed"
                rm $tmp_file
                return $exit_failure
        fi
	#Check for the length of ME_ID
        ME_ID=$(echo ${#result})
        if [ $ME_ID -eq 0 ]; then
                flog "$result is not defined"
        elif [ $ME_ID -gt 8 ]; then
                #echo "APG name doesn't have at least 7 characters"
                flog "$result is more than 8 characters"
                rm $tmp_file
                return $exit_failure
	else
                flog "Length of the node name "$result" is $ME_ID"

        fi
	#Check for the case of ME_IDi.,e APG anme
        test=`echo $result | $GREP [a-z]`
        if [ $? -ne 0 ]; then
              # echo -e "\nCheck to verify if APG name is uppercase or not is verified"
                flog "$result is uppercase"
        else
               #echo "APG name is in lowercase"
                flog "$result is in lowercase"
                rm $tmp_file
                return $exit_failure

        fi
        flog "Check to verify node name succeeded"
        flog "Healthcheck succeeded"



        rm $tmp_file

        trace_leave $FUNCNAME
        return $exit_success
}
#----------------------------------------------------------
function IO_HCRS_36(){
	if [ $a_local == $TRUE ]; then #  If '-l' option is specified, rule should not be executed.
        	return $exit_success
	else

        trace_enter $FUNCNAME

        remote_node=`$CAT $peer_node_file`
        tmp_file=`mktemp -t tempfile.XXXXXX`
        tmp_file2=`mktemp -t tempfile.XXXXXX`
        tmp_file3=`mktemp -t tempfile.XXXXXX`
        tmp_file4=`mktemp -t tempfile.XXXXXX`


        launchCommand1="rpm -qa"
        launchCommand2="$CMW_REPOSITORY --node SC-2-1"
        launchCommand3="$CMW_REPOSITORY --node SC-2-2"


        $launchCommand1 2>/dev/null > $tmp_file
	      ${SSH} -n $remote_node $launchCommand1 2>/dev/null  > $tmp_file2
        if [ $remote_node == "SC-2-2" ]; then
           #AIT-TA RPM will be installed on SC-2-2 of vAPG.
           $SED -i '/ait-ta/d' $tmp_file2
        else
           $SED -i '/ait-ta/d' $tmp_file
        fi
        diff --brief <(sort $tmp_file) <(sort $tmp_file2) >/dev/null 2>&1
        comp_value=$?
        if [ $comp_value -eq 1 ]; then
                        flog "Installed rpms versions on both nodes are different"
                        rm $tmp_file $tmp_file2
                        return $exit_failure
        else
                        flog "Installed rpm versions on both nodes are same"
        fi

        $launchCommand2 2>/dev/null > $tmp_file
        $launchCommand3 2>/dev/null > $tmp_file2

        parsePrintout1="$CAT $tmp_file | $AWK '{print \$2}'"
        parsePrintout2="$CAT $tmp_file2 | $AWK '{print \$2}'"

        eval $parsePrintout1 > $tmp_file3
        eval $parsePrintout2 > $tmp_file4
	        diff --brief <(sort $tmp_file3) <(sort $tmp_file4) >/dev/null 2>&1
        comp_value=$?

        if [ $comp_value -eq 1 ]; then
                        flog "Installed sdp versions on both nodes are different"
                        return $exit_failure
                        return $exit_failure
        else
                        flog "Installed sdp versions on both nodes are same"
        fi


        flog "Check to verify software installed on node succeeded"
        flog "Healthcheck succeeded"

        rm $tmp_file $tmp_file2 $tmp_file3 $tmp_file4
        trace_leave $FUNCNAME
        return $exit_success
	fi
}
#----------------------------------------------------------
function IO_HCRS_37(){

        trace_enter $FUNCNAME
        swMId=
        exitflag=0
        min=3
        max=5
        launchCommand="$IMMLIST CmwSwIMswInventoryId=1"
        eval $launchCommand > /dev/null 2>&1
        if [ $? -eq 0 ]; then
                swMId="CmwSwIMswInventoryId=1"
        else
                swMId="swInventoryId=1"
        fi
        #Fetch the ActiveSwVersion list
        activeswversion_file=`mktemp -t tempfile.XXXXXX`
        while [ $min -le $max ]
        do
                launchCommand1="$IMMLIST $swMId | $GREP -i ^active | $AWK '{print \$$min}' | $CUT -d ',' -f1 | $CUT -d "=" -f2"
                activeswversion=`eval $launchCommand1`
                if [ -z "$activeswversion" ];then
                        break
                else
                        $ECHO $activeswversion 2>/dev/null >> $activeswversion_file
                        min=$max
                        max=$(( $min + 2 ))
                fi
        done
        #Fetch the productName list
        id=`$IMMFIND -c CmwSwIMProductData | $GREP -i swVersionId`
        for i in $id
        do
                product=`$IMMLIST $i | $GREP -i ^productName | $AWK '{print \$3}'`
                #Fetch the number of Swversion instances
                N_Sw_Ver=`$IMMFIND | $GREP -i ^swVersionId | $GREP -i -w "\b$product\b" | $WC -l`
                #Check the case: Number of SwVersion for the $product is greater than 1
                if [[ $N_Sw_Ver -gt 1 ]]; then
                        N_UP_entry=`$IMMFIND | $GREP -i ^UpgradePackage | $GREP -i $product | $WC -l`
                        #Checking if Upinstance exists. If Upinstance exists, swversion duplicate instance could be valid
                if [[ $N_UP_entry -ne 0 ]]; then
                                #UpInstance is present, must be check if un Upgrade is ongoing.
                            flog "UpgradePackage instance is not removed for $product"
				tmp_file=`mktemp -t tempfile.XXXXXX`
                                launchCommand="$IMMFIND | $GREP -i ^UpgradePackage | $GREP -i $product"
                            eval $launchCommand 2>/dev/null > $tmp_file
                        up_flag=0
                                while read cLine
                            do
                                    state=`$IMMLIST $cLine | $GREP -i ^state | $AWK '{print \$3}'`
                                        if [ $state -ne 7 ]; then
                                                up_flag=1
                                        break
                                        fi
                                done <$tmp_file
                                rm $tmp_file
                                if [ $up_flag -eq 1 ];then
                                        #Upgrade is on going
                                        flog "Upgrade is in progress for $product"
                                else
                                        #More than one swVersion is present for the $product. Also a UpInstance is present and no Upgrade is ongoing.
                                        flog "More than one swVersionId instances found in MOM SwInventory for $product"
                                        exitflag=1
                                        #We have found a problem, APG is NOT Healty, exit from cycle
                                        break
                                fi
                        else
                                        #More than one swVersion is present for the $product but no one UpInstance is present
                                        flog "More than one swVersionId instances found in MOM SwInventory for $product"
                                        exitflag=1
                                        #We have found a problem, APG is NOT Healty, exit from cycle
                                        break
                        fi
                else
                        #Check the case: Number of SwVersion for the $product is equal to 1.
                        #If for a $product is present only one SwVersion, the SwVersion must be active.
                        aflag=0
                        swversion=`$IMMFIND | $GREP -i ^swVersionId | $GREP -i -w "\b$product\b" | $CUT -d ',' -f1 | $CUT -d "=" -f2`
                        #Check if the swVersion is an ActiveSwVersion.
                        while read cLine
                        do
                                if [ "$swversion" == "$cLine"  ]; then
                                        activeswversion=$swversion
                                        aflag=1
					break
                                fi
                done <$activeswversion_file

                        if [ $aflag -eq 1 ]; then
                                #ActiveSwVersion found
                                flog "The activeSwversion for product $product is $activeswversion."
                        else
                                #ActiveSwVersion NOT found!!!
                                flog "No activeSwVersion present for product $product."
                                exitflag=1
                                #We have found a problem, APG is NOT Healty, exit from cycle
                                break
                        fi
                fi
        done
        rm $activeswversion_file
        #We have found a problem, APG is NOT Healty, exit from cycle
        if [ $exitflag -eq 1 ]; then
                return $exit_failure
        fi

    flog "Check to verify swversionID instances before Software update succeeded"
    flog "Healthcheck succeeded"
    trace_leave $FUNCNAME
    return $exit_success
}

#---------------------------------------------------------------------------------
function IO_HCRS_38(){
	trace_enter $FUNCNAME
  N_UP_entry=`$IMMFIND | $GREP -i ^UpgradePackage |$WC -l`
  if [[ $N_UP_entry -ne 0 ]]; then
		tmp_file=`mktemp -t tempfile.XXXXXX`
    launchCommand="$IMMFIND | $GREP -i ^UpgradePackage"
    eval $launchCommand 2>/dev/null > $tmp_file
    up_flag=0
    while read cLine
    do
			UP_instance=`$IMMLIST $cLine | $GREP -i ^upgradePackageId | $AWK '{print \$3}' | $CUT -d '=' -f2`
     	state=`$IMMLIST $cLine | $GREP -i ^state | $AWK '{print \$3}'`
     	if [ $state -ne 7 ]; then
				flog "Upgrade is in progress"
			else
       	flog "UpgradePackage instance $UP_instance  has to be removed"
				up_flag=1
			fi
			done <$tmp_file
			rm $tmp_file
			if [ $up_flag -eq 1 ];then
				return $exit_failure
			fi
	else
  flog "No traces of UpgradePackage instance found"
  fi
	flog "Check to verify upgrade package instances before Software update succeeded"
	flog "Healthcheck succeeded"
	trace_leave $FUNCNAME
	return $exit_success
}

#BEGIN - xanttro - 01-05-2014 HCSTART RESTORE CHECK 
#---------------------------------------------------------------------------------
function IO_HCRS_39(){
	trace_enter $FUNCNAME
	###########################    STS Folder Chech       ############################
	#************************************************************************************
		
	tmp_file=`mktemp -t tempfile.XXXXXX`
	tmp_file2=`mktemp -t tempfile.XXXXXX`
	STS_FOLDER="/data/opt/ap/internal_root/data_transfer/source/sts"
	failure=0		

	firstShowFolder=0
	firstWarningMessage=0
	
	launchCommand="$IMMFIND | $GREP -i measurementprogramid "	
	eval $launchCommand > $tmp_file	 
	launchCommand2="$LS -l --time-style=\"long-iso\" $STS_FOLDER | $EGREP ^d | $AWK '{print \$8}'"
	
	eval $launchCommand2 > $tmp_file2	
	for line2 in $($CAT $tmp_file2)  #STS FOLDER 
	
	do
		findit=0
		for line in $($CAT $tmp_file) 
		
		do
			TQdefined=`$IMMLIST $line | $GREP transferQueue| $AWK '{print $3}'`
			#echo "TQ definita: $TQdefined"
			if [ -n $TQdefined ]; then
				#echo "line2: $TQdefined"
				if [ $line2 == "$TQdefined" ]; then
					findit=1
					break
				else
					findit=0						
				fi
			fi
		done
		if [ $findit -eq 0 ]; then
			#check if void folder
			 folderCheck=`find "$STS_FOLDER/$line2" -maxdepth 0 -empty`  #if empty, return folder name
			if [ $folderCheck ]; then
				#empty folder, can delete it
					flog "Empty folder $STS_FOLDER/$line2 without Transfer Queue removed"
					`rm -rf $STS_FOLDER/$line2`
			else
				if [ $a_verbose == $TRUE ]; then
					if [ $firstWarningMessage -eq 0 ]; then

						$ECHO "$reason: $exportSlogan"
						$ECHO ""
						firstWarningMessage=1
					fi

					if [ $firstShowFolder -eq 0 ]; then
						$ECHO "List of unused folder(s) related to missing transfer queue(s):"
					fi
					firstShowFolder=1
					$ECHO "$STS_FOLDER/$line2 "
				fi
				flog "Not empty folder $STS_FOLDER/$line2 "
				failure=1
				#return $exit_failure
			fi
		#else
			#Folder with Transfer Queue. nothing to do
		fi			
	done
	 
	#$ECHO ""	
	rm $tmp_file
	rm $tmp_file2
	line2=''
	line=''
	findit=0
	###########################    CP Folder Chech   ############################
	#************************************************************************************
	tmp_file=`mktemp -t tempfile.XXXXXX`
	tmp_file2=`mktemp -t tempfile.XXXXXX`
	tmp_file3=`mktemp -t tempfile.XXXXXX`
        tmp_file4=`mktemp -t tempfile.XXXXXX`
        tmp_file5=`mktemp -t tempfile.XXXXXX`	
	CP_FOLDER="/data/opt/ap/internal_root/data_transfer/source/cp_file"
	
	#CHECK ADH installed for CP verify
	isADH=`$IMMFIND | $GREP AxeCpFileSystemcpFileSystemMId=1 | $GREP -v cpVolumeId`
	if [ $isADH ]; then
		#with this, we have a list of CpVolume
		launchCommand="$IMMFIND -s sublevel AxeCpFileSystemcpFileSystemMId=1 | $GREP -i cpvolumeid"  			
		eval $launchCommand > $tmp_file	 
		
		#go to /data/opt/ap/internal_root/data_transfer/source/cp_file and get all TQ folder. 
		launchCommand3="$LS -l --time-style=\"long-iso\" $CP_FOLDER | $EGREP ^d | $AWK '{print \$8}'"	
		eval $launchCommand3 > $tmp_file3      #CP folder list 
	        compositeFileName=""	
		for line3 in $($CAT $tmp_file3)  #for each CP folder list
		do
		
			for line in $($CAT $tmp_file)  #for each CpVolume 
				
			do	
                                launchCommand4="$IMMFIND -s sublevel $line | $GREP -i CompositeFileid"
				eval $launchCommand4 > $tmp_file4
				for line5 in $($CAT $tmp_file4)
				do
					compositeFileName=`$IMMLIST $line5 | $GREP -i CompositeFileid | $AWK '{print $3}' | $CUT -d = -f2` 
					if [ -n $compositeFileName ]; then
                                                        if [ $line3 == "$compositeFileName" ]; then
                                                                findit=1
                                                                break 2
                                                        else
                                                                findit=0
                                                        fi
                                         fi

				done

				#search the InfiniteFile
				launchCommand2="$IMMFIND -s sublevel $line | $GREP -i InfiniteFileid " 
				eval $launchCommand2 > $tmp_file2
				
				for line2 in $($CAT $tmp_file2)   #for each InfiniteFile
				
				do 
					#get TransferQueue Value 
#BEGIN - XANTTRO - 24/06/2014 - HCSTART error in CP folder check 					
					TQdefined=`$IMMLIST $line2 | $GREP transferQueue | $GREP -v transferQueueDn | $AWK '{print $3}'`
#END - XANTTRO - 24/06/2014 - HCSTART error in CP folder check					
						#echo "TQ definita: $TQdefined"
						if [ -n $TQdefined ]; then
							#echo "line2: $TQdefined"
							if [ $line3 == "$TQdefined" ]; then
								findit=1
								break 2
							else
								findit=0						
							fi
						fi
				done
			done
			
			if [ $findit -eq 0 ]; then
				#check if void folder
				 folderCheck=`find "$CP_FOLDER/$line3" -maxdepth 0 -empty`  #if empty, return folder name
				if [ $folderCheck ]; then
					#void folder, can delete it
						flog "Empty folder $CP_FOLDER/$line3 without Transfer Queue removed"
						`rm -rf $CP_FOLDER/$line3`
				 else
						if [ $a_verbose == $TRUE ]; then
							if [ $firstWarningMessage -eq 0 ]; then
								$ECHO "$reason: $exportSlogan"
								$ECHO ""
								firstWarningMessage=1
							fi
							if [ $firstShowFolder -eq 0 ]; then
								$ECHO "List of unused folder(s) related to missing transfer queue(s):"
							fi
							firstShowFolder=1
							$ECHO "$CP_FOLDER/$line3 "
						fi
						flog "Not empty folder $CP_FOLDER/$line3 "
						failure=1
						#return $exit_failure
				 fi
			#else
				#Folder with Transfer Queue. nothing to do
			fi		
			done
	fi
	rm $tmp_file
	rm $tmp_file2
	rm $tmp_file3
	
###########################    PDS Folder Chech       ############################
#************************************************************************************
		

	tmp_file=`mktemp -t tempfile.XXXXXX`
	tmp_file2=`mktemp -t tempfile.XXXXXX`
	PDS_FOLDER="/data/opt/ap/internal_root/data_transfer/source/cp_printouts"
			
	if [ -d "$PDS_FOLDER" ]; then
		launchCommand="$IMMFIND | $GREP -i pdssubfileid "	
		eval $launchCommand > $tmp_file	 
		
		launchCommand="$IMMFIND | $GREP -i PdsStandByDevice "	
		eval $launchCommand >> $tmp_file	 
		
		
		launchCommand2="$LS -l --time-style=\"long-iso\" $PDS_FOLDER | $EGREP ^d | $AWK '{print \$8}'"
		eval $launchCommand2 > $tmp_file2	
		
		for line2 in $($CAT $tmp_file2)  #PDS FOLDER 
		
		do
			findit=0
			for line in $($CAT $tmp_file)  #for each PdsSubFile
			
			do 
				#get TransferQueue Value 
				TQdefined=`$IMMLIST $line | $GREP transferQueue| $GREP -v transferQueueDn| $AWK '{print $3}'`
					#echo "TQ definita: $TQdefined"
					if [ -n $TQdefined ]; then
						
						if [ $line2 == "$TQdefined" ]; then
							findit=1
							break
						else
							findit=0						
						fi
					fi
			done
			
			if [ $findit -eq 0 ]; then
				#check if void folder
				 folderCheck=`find "$PDS_FOLDER/$line2" -maxdepth 0 -empty`  #if empty, return folder name
				if [ $folderCheck ]; then
					#empty folder, can delete it
						flog "Empty folder $PDS_FOLDER/$line2 without Transfer Queue removed"
						`rm -rf $PDS_FOLDER/$line2`
				else
					if [ $a_verbose == $TRUE ]; then
						if [ $firstWarningMessage -eq 0 ]; then

							$ECHO "$reason: $exportSlogan"
							$ECHO ""
							firstWarningMessage=1
						fi

						
						if [ $firstShowFolder -eq 0 ]; then
							$ECHO "List of unused folder(s) related to missing transfer queue(s):"
						fi
						firstShowFolder=1
						$ECHO "$PDS_FOLDER/$line2 "
					fi
					flog "Not empty folder $PDS_FOLDER/$line2 "
					failure=1
					#return $exit_failure

				fi
			#else
				#Folder with Transfer Queue. nothing to do

			fi			
		done
	fi
	
	# if [ $firstWarningMessage -eq 1 ]; then
		# $ECHO ""
		# $ECHO "Recommended Action: Transfer all files in each shown folder to a remote system by following OPI \"AP , File, Transfer\"."
		# $ECHO "Afterwards all folders(s) can be removed."
	# fi
	#$ECHO ""	
	rm $tmp_file
	rm $tmp_file2
#BEGIN - XANTTRO - 24/06/2014 - HCSTART HealthCheck Folder Chech
###########################    HealthCheck Folder Chech       ############################
#************************************************************************************
		

	tmp_file=`mktemp -t tempfile.XXXXXX`
	tmp_file2=`mktemp -t tempfile.XXXXXX`
	HC_FOLDER="/data/opt/ap/internal_root/data_transfer/source/health_check"
			
	if [ -d "$HC_FOLDER" ]; then
		launchCommand="$IMMFIND | $GREP jobId | $GREP -v jobSchedulerId | $GREP -v id="	
		eval $launchCommand > $tmp_file	 
		
		launchCommand2="$LS -l --time-style=\"long-iso\" $HC_FOLDER | $EGREP ^d | $AWK '{print \$8}'"
		eval $launchCommand2 > $tmp_file2	
		
		for line2 in $($CAT $tmp_file2)  #HC FOLDER 
		
		do
			findit=0
			for line in $($CAT $tmp_file)  #for each Job 
			
			do 
				#get TransferQueue Value 
				TQdefined=`$IMMLIST $line | $GREP destinationUri | $AWK '{print $3}'`
					#echo "TQ definita: $TQdefined"
					if [ -n $TQdefined ]; then
						
						if [ $line2 == "$TQdefined" ]; then
							findit=1
							break
						else
							findit=0						
						fi
					fi
			done
			
			if [ $findit -eq 0 ]; then
				#check if void folder
				 folderCheck=`find "$HC_FOLDER/$line2" -maxdepth 0 -empty`  #if empty, return folder name
				if [ $folderCheck ]; then
					#empty folder, can delete it
						flog "Empty folder $HC_FOLDER/$line2 without Transfer Queue removed"
						`rm -rf $HC_FOLDER/$line2`
				else
					if [ $a_verbose == $TRUE ]; then
						if [ $firstWarningMessage -eq 0 ]; then

							$ECHO "$reason: $exportSlogan"
							$ECHO ""
							firstWarningMessage=1
						fi

						
						if [ $firstShowFolder -eq 0 ]; then
							$ECHO "List of unused folder(s) related to missing transfer queue(s):"
						fi
						firstShowFolder=1
						$ECHO "$HC_FOLDER/$line2 "
					fi
					flog "Not empty folder $HC_FOLDER/$line2 "
					failure=1
					#return $exit_failure

				fi
			#else
				#Folder with Transfer Queue. nothing to do

			fi			
		done
	fi
	
	# if [ $firstWarningMessage -eq 1 ]; then
		# $ECHO ""
		# $ECHO "Recommended Action: Transfer all files in each shown folder to a remote system by following OPI \"AP , File, Transfer\"."
		# $ECHO "Afterwards all folders(s) can be removed."
	# fi
	#$ECHO ""	
	rm $tmp_file
	rm $tmp_file2

#END - XANTTRO - 24/06/2014 - HCSTART HealthCheck Folder Chech
	
################    Cleanup unused RTR related files/folders in APG file system after Restore #######################
#************************************************************************************
	tmp_file=`mktemp -t tempfile.XXXXXX`
	tmp_file2=`mktemp -t tempfile.XXXXXX`
	tmp_file3=`mktemp -t tempfile.XXXXXX`
	tmp_file4=`mktemp -t tempfile.XXXXXX`
	

	MESSAGESTORE_FOLDER="/data/opt/ap/internal_root/data_transfer/source/data_records"
	firstDS=0
	launchCommand="$IMMFIND | $GREP -i filebasedjobid | $GREP -vi statisticsinfoid"  			
	eval $launchCommand > $tmp_file	 
	for line in $($CAT $tmp_file)   #for each 
			
	do 
		trasferQueueRTR=`$IMMLIST $line | $GREP transferQueue | $AWK '{print $3}'`
		`$ECHO $trasferQueueRTR >> $tmp_file2`
	done
	

	messageStoreCheck=`find "$MESSAGESTORE_FOLDER" -maxdepth 0 -empty`  #if empty, return folder name
	if [ -z $messageStoreCheck ]; then
	
		MESSAGESTORE_FOLDER=`$ECHO "$MESSAGESTORE_FOLDER/transfer_store"`
            
                if [ -d "$MESSAGESTORE_FOLDER" ]; then
			#go to /data/opt/ap/internal_root/data_transfer/source/data_records/transfer_store and get all TQ folder. 
			launchCommand3="$LS -l --time-style=\"long-iso\" $MESSAGESTORE_FOLDER | $EGREP ^d | $AWK '{print \$8}'"	
			eval $launchCommand3 > $tmp_file3      #MessageStore folder list 
        
                        if [ -s "$tmp_file3" ]; then
				for line3 in $($CAT $tmp_file3)  #for each CP folder list
				do	
	
					#go to /data/opt/ap/internal_root/data_transfer/source/data_records/transfer_store/[MESSAGESTORE_FOLDER]/[TQ] and get all TQ folder. 
					launchCommand4="$LS -l --time-style=\"long-iso\" $MESSAGESTORE_FOLDER/$line3 | $EGREP ^d | $AWK '{print \$8}'"	
					eval $launchCommand4 > $tmp_file4      #TQ folder list 
			

					for line4 in $($CAT $tmp_file4)  #for each TQ folder list
					do
						findit=0
						for line5 in $($CAT $tmp_file2)   #for each TQ, check...

						do 
							if [ $line4 == $line5 ]; then
								#$ECHO "folder trovata"
								findit=1
								break
							fi
					
						done
						if [ $findit -eq 0 ]; then
							#check if void folder
							folderCheck=`find "$MESSAGESTORE_FOLDER/$line3/$line4" -maxdepth 0 -empty`  #if empty, return folder name
							if [ $folderCheck ]; then
							#void folder, can delete it
								flog "Empty folder $MESSAGESTORE_FOLDER/$line3/$line4 without Transfer Queue removed"
								`rm -rf $MESSAGESTORE_FOLDER/$line3/$line4`
							else
								if [ $a_verbose == $TRUE ]; then
									if [ $firstWarningMessage -eq 0 ]; then
										$ECHO "$reason: $exportSlogan"
										$ECHO ""
										firstWarningMessage=1
									fi
									if [ $firstShowFolder -eq 0 ]; then
										$ECHO "List of unused folder(s) related to missing transfer queue(s):"
									fi
									firstShowFolder=1
									$ECHO "$MESSAGESTORE_FOLDER/$line3/$line4 "
								fi
								flog "Not empty folder $MESSAGESTORE_FOLDER/$line3/$line4 "
								failure=1
						
							fi
						fi
					done

				done	
                	fi
		fi
        fi
	
	if [ $firstShowFolder -eq 1 ]; then
		$ECHO ""
	fi
	
	
	rm $tmp_file
	rm $tmp_file2
	rm $tmp_file3
	rm $tmp_file4
	#$ECHO ""


################    Cleanup unused Destination Sets related files/folders in APG file system after Restore#######################
#************************************************************************************
	tmp_file=`mktemp -t tempfile.XXXXXX`
	tmp_file2=`mktemp -t tempfile.XXXXXX`
	tmp_file3=`mktemp -t tempfile.XXXXXX`
	PREFIX_FOLDER="/data/opt/ap/internal_root/data_transfer/destinations/"

	firstShowDestSet=0
	launchCommand="$IMMFIND | $GREP -i filetransferqueueid | $GREP -v advancedFileTransferQueueInfoId"  			
	eval $launchCommand > $tmp_file	 
	for line in $($CAT $tmp_file)   #for each 
			
	do 
		#BEGIN - XANTTRO - 10/06/2014 - ISSUE 27 - HCSTART error in destinations folder check (CDH)
		#get 
		respondingDestinationSetDn=`$IMMLIST $line | $GREP respDestinationSetDn | $AWK '{print $3}'`
		initiatingDestinationSet=`$IMMLIST $line | $GREP initiatingDestinationSet| $GREP -v Dn | $AWK '{print $3}'`
		#fileTransferQueueId=`$IMMLIST $line | $GREP fileTransferQueueId| $AWK '{print $5}'`	
		
		if [ "$initiatingDestinationSet" != "<Empty>" ]; then
			#$ECHO "initiatingDestinationSet:  $initiatingDestinationSet "
			`$ECHO $initiatingDestinationSet >> $tmp_file3`
		fi	
		if [ "$respondingDestinationSetDn" != "<Empty>" ]; then
			#$ECHO "respondingDestinationSetDn: $respondingDestinationSet"
			respActiveDestination=`$IMMLIST $respondingDestinationSetDn | $GREP respActiveDestination | $AWK '{print $3}'`
			`$ECHO $respActiveDestination >> $tmp_file3`
			
		fi
		#END - XANTTRO - 10/06/2014 - ISSUE 27 - HCSTART error in destinations folder check (CDH)

	done
	line2='' 
	line=''
	findit=0
	launchCommand2="$LS -l --time-style=\"long-iso\" $PREFIX_FOLDER | $EGREP ^d | $AWK '{print \$8}'"  #list of destination folder
	eval $launchCommand2 > $tmp_file2	
	for line2 in $($CAT $tmp_file2)   #for each folder, check...

	do 
		findit=0
		for line in $($CAT $tmp_file3)   #for each destinationset, check...

		do 

			if [ $line == $line2 ]; then
				#$ECHO "folder trovata"
				findit=1
				break
			fi
			
		done
		if [ $findit -eq 0 ]; then
			#check if void folder
			folderCheck=`find "$PREFIX_FOLDER/$line2" -maxdepth 0 -empty`  #if empty, return folder name
			if [ $folderCheck ]; then
			#void folder, can delete it
				flog "Empty folder $PREFIX_FOLDER$line2 without Transfer Queue removed"
				`rm -rf $PREFIX_FOLDER$line2`
			else
				if [ $a_verbose == $TRUE ]; then
					if [ $firstWarningMessage -eq 0 ]; then

						$ECHO "$reason: $exportSlogan"
						$ECHO ""
						firstWarningMessage=1
					fi

					if [ $firstShowDestSet -eq 0 ]; then
						$ECHO "List of unused folder(s) related to missing destination(s):"
					fi
					firstShowDestSet=1
					$ECHO "$PREFIX_FOLDER$line2 "
				fi
				flog "Not empty folder $PREFIX_FOLDER$line2 "
				failure=1
				
			fi
		fi
	done
	
	if [ $firstShowDestSet -eq 1 ]; then
		$ECHO ""
	fi
	rm $tmp_file
	rm $tmp_file2
	rm $tmp_file3	

	trace_leave $FUNCNAME
	if [ $failure -eq 0 ]; then
		return $exit_success
	else
		return $exit_failure
	fi
	
}	

function IO_HCRS_40(){
	trace_enter $FUNCNAME
	
	tmp_file=`mktemp -t tempfile.XXXXXX`
	tmp_file2=`mktemp -t tempfile.XXXXXX`
	firstShowPack=0
	failure=0
	apgShelfArchitecture=`$IMMLIST axeFunctionsId=1 | $GREP apgShelfArchitecture | $AWK '{print $3}'`
	#Is an Enhanced Generic Ericsson Magazine 2 (EGEM2) the magazine type the APG is located into? 
	#0 = SCB
	#1 = SCX
	if [ $apgShelfArchitecture == 1 ]; then
		launchCommand="$IMMFIND | $GREP -i apmanagedswpackageid | $GREP -v id= | $GREP -v reportProgressId"
		eval $launchCommand > $tmp_file	
		for line in $($CAT $tmp_file)   #for each apmanagedswpackageid, check...

		do 
			packageStatus=`$IMMLIST $line | $GREP packageStatus | $AWK '{print $3}'`
			apManagedSwPackageId=`$IMMLIST $line | $GREP -v id= | $GREP apManagedSwPackageId | $AWK '{print $3}'`
			
			if [ $packageStatus != 0 ]; then  #packageStatus 0 = VALID
				failure=1
				posIndex=`expr index "$apManagedSwPackageId" 1=`				
				if [ $a_verbose == $TRUE ]; then
					if [ $firstShowPack -eq 0 ]; then
						#$ECHO ""
						$ECHO "List of missing blade SW package(s):"
					fi
					firstShowPack=1
					$ECHO ${apManagedSwPackageId:$posIndex}
					#$ECHO "$apManagedSwPackageId"
				fi
				flog "Faulty Marked Blade SW package $apManagedSwPackageId"
			fi
		done
		
		
		launchCommand2="$IMMFIND | $GREP -i cpmanagedswpackageid | $GREP -v id= | $GREP -v reportProgressId"
		eval $launchCommand2 > $tmp_file2	
		for line2 in $($CAT $tmp_file2)   #for each cpmanagedswpackageid, check...

		do 
			packageStatus=`$IMMLIST $line2 | $GREP packageStatus | $AWK '{print $3}'`
			cpManagedSwPackageId=`$IMMLIST $line2 | $GREP -v id= | $GREP cpManagedSwPackageId | $AWK '{print $3}'`
			
			if [ $packageStatus != 0 ]; then
				failure=1
				posIndex=`expr index "$cpManagedSwPackageId" 1=`
				if [ $a_verbose == $TRUE ]; then
					if [ $firstShowPack -eq 0 ]; then
						#$ECHO ""
						$ECHO "List of missing blade SW package(s):"
					fi
					firstShowPack=1
					
					$ECHO ${cpManagedSwPackageId:$posIndex}
					#$ECHO "$cpManagedSwPackageId"
					
				fi
				flog "Faulty Marked Blade SW package $cpManagedSwPackageId "
			fi
		done



		$ECHO ""	
	fi
	
	rm $tmp_file2
	rm $tmp_file
	
	trace_leave $FUNCNAME
	if [ $failure -eq 0 ]; then
		return $exit_success
	else
		return $exit_failure
	fi
	
}
#END - xanttro - 01-05-2014 HCSTART RESTORE CHECK 
#---------------------------------------------------------------------------------
function IO_HCRS_41(){

        trace_enter $FUNCNAME
        launchCommand="$CAT /etc/bash.bashrc.local  | $GREP \"PS1\" | $GREP \"#\" | $AWK -F'-' '{print \$1}' | $CUT -d= -f 2 | $SED -e 's/.//'"
        tmp_file=`mktemp -t tempfile.XXXXXX`

        #Checking Bash Prompt on Node A
        eval $launchCommand > $tmp_file
        nodeAName=`$CAT $tmp_file`

        #Checking Bash Prompt on Node B
        check_peer_node_state
        if [ $? == 0 ]; then
                # Launch command on remote node
                tmp_file2=`mktemp -t tempfile.XXXXXX`
                ${SSH} -n $remote_node $launchCommand 2>/dev/null > $tmp_file2
                nodeBName=`$CAT $tmp_file2`

                if [ $nodeAName != "$nodeBName" ]; then
                        flog "Bash Prompt with new Node Name is not similar on both the nodes"
                        rm $tmp_file $tmp_file2
                        return $exit_failure
                else
                        flog "Bash Prompt with new Node Name is similar on both the nodes"
                fi
        else
                flog "Remote node is not up. Health not checked on remote node"
            	rm $tmp_file $tmp_file2
                return $exit_failure
        fi
        rm $tmp_file $tmp_file2
        trace_leave $FUNCNAME
        return $exit_success
}
#---------------------------------------------------------------------------------

function IO_HCRS_42(){

	trace_enter $FUNCNAME
	launchCommand1="$ALIST -s A1"
	output="No match found."
        tmp_file=`mktemp -t tempfile.XXXXXX`
	tmp_file2=`mktemp -t tempfile.XXXXXX`
	parsePrintout="$CAT $tmp_file"
	# Checking health on local node
        eval $launchCommand1 > $tmp_file
        result=`eval $parsePrintout`
        checkHealth "local" $FUNCNAME
	if [ $? == $exit_failure ]; then
                alarm=$($CAT $tmp_file | $GREP "Alarm Identifier" -A 2 | $CUT -d " " -f 1  | $GREP -v ^Alarm | $GREP [0-9])
                for i in $alarm
                do
				#Checking if ceasing alarms are present or not
                        launchCommand3="$ALIST -i $i"
                        eval $launchCommand3 > $tmp_file2
                        $CAT $tmp_file2 | $GREP -w CEASING >/dev/null
                        if [ $? == 0 ]; then
                                flog "Ceasing alarm exists with alarm identifier $i"
                        else
				flog "One or more A1 alarms active on local node"
                                rm $tmp_file $tmp_file2
                                return $exit_failure
                        fi
                done

        fi
                flog "No severe A1 alarms active on local node"
	        flog "Healthcheck succeeded on local node"
		# Check if -l option is specified
        if [ $a_local == $FALSE ]; then # If no -l option is specified, then check health of remote node

                # Checking status of peer node
                check_peer_node_state
                if [ $? == 0 ]; then
                        # Launch command on remote node
                        ${SSH} -n $remote_node $launchCommand1 2>/dev/null > $tmp_file
                        result=`eval $parsePrintout`
                        checkHealth "remote" $FUNCNAME
                        if [ $? == $exit_failure ]; then
                                alarm=$($CAT $tmp_file | $GREP "Alarm Identifier" -A 2 | $CUT -d " " -f 1  | $GREP -v ^Alarm | $GREP [0-9])
                                for i in $alarm
                                do
                                        #Checking if ceasing alarms are present
					launchCommand3="$ALIST -i $i"
                                        ${SSH} -n $remote_node $launchCommand3 2>/dev/null > $tmp_file2
                                        $CAT $tmp_file2 | $GREP -w CEASING >/dev/null
                                        if [ $? == 0 ]; then
                                                flog "Ceasing alarm exists with alarm identifier $i on remote node"
                                        else
					        flog "One or more A1 alarms active on remote node"
                                                rm $tmp_file $tmp_file2
                                                return $exit_failure
                                        fi
                                done

                        fi
				flog "No severe A1 alarms active on remote node"
				flog "Healthcheck succeeded on remote node"
		        else
                                flog "Remote node is not up. Health not checked on remote node"
			fi
		fi
		
	rm $tmp_file $tmp_file2

	trace_leave $FUNCNAME
	return $exit_success
	
}
		
function IO_HCRS_43(){
		
	trace_enter $FUNCNAME
	launchCommand2="$ALIST -s A2"
	output="No match found."
        tmp_file=`mktemp -t tempfile.XXXXXX`
	tmp_file2=`mktemp -t tempfile.XXXXXX`
	tmp_file3=`mktemp -t tempfile.XXXXXX`
        parsePrintout="$CAT $tmp_file"
	eval $launchCommand2 > $tmp_file
        result=`eval $parsePrintout`
        checkHealth "local" $FUNCNAME
        if [ $? == $exit_failure ]; then
                alarm=$($CAT $tmp_file | $GREP "Alarm Identifier" -A 2 | $CUT -d " " -f 1  | $GREP -v ^Alarm | $GREP [0-9])
                for i in $alarm
                do
			#Checking if ceaseing alarms are present or not
                        launchCommand3="$ALIST -i $i"
                        eval $launchCommand3 > $tmp_file3
                        $CAT $tmp_file3 | $GREP -w CEASING >/dev/null
                        if [ $? == 0 ]; then
                                flog "Ceasing alarm exists with alarm identifier $i"
                        else
				flog "One or more A2 alarms active on local node"
                                rm $tmp_file $tmp_file2 $tmp_file3
                                return $exit_failure
                        fi
                done
        fi
        	flog "No severe A2 alarms active on local node"
		flog "Healthcheck succeeded on local node"
		# Check if -l option is specified
        if [ $a_local == $FALSE ]; then # If no -l option is specified, then check health of remote node

                # Checking status of peer node
                check_peer_node_state
                if [ $? == 0 ]; then
			${SSH} -n $remote_node $launchCommand2 2>/dev/null > $tmp_file
                        result=`eval $parsePrintout`
                        checkHealth "remote" $FUNCNAME
                        if [ $? == $exit_failure ]; then
                                alarm=$($CAT $tmp_file | $GREP "Alarm Identifier" -A 2 | $CUT -d " " -f 1  | $GREP -v ^Alarm | $GREP [0-9])
                                for i in $alarm
                                do
                                        #Checking if ceasing alarms are present
                                        launchCommand3="$ALIST -i $i"
                                        ${SSH} -n $remote_node $launchCommand3 2>/dev/null > $tmp_file2
                                        $CAT $tmp_file2 | $GREP -w CEASING >/dev/null
                                        if [ $? == 0 ]; then
                                                flog "Ceasing alarm exists with alarm identifier $i on remote node"
                                        else
						flog "One or more A2 alarms active on remote node"
                                                rm $tmp_file $tmp_file2 $tmp_file3
                                                return $exit_failure
                                        fi
                                done

                        fi
        		
				flog "No severe A2 alarms active on remote node"
				flog "Healthcheck succeeded on remote node"
	
			else
                    		flog "Remote node is not up. Health not checked on remote node"
			fi
	fi
		
	rm $tmp_file $tmp_file2 $tmp_file3
        trace_leave $FUNCNAME
	return $exit_success
}


function IO_HCRS_44(){
		
	trace_enter $FUNCNAME
	local node_id=$(< /etc/cluster/nodes/this/id)
	local cmd_hwtype='/opt/ap/apos/conf/apos_hwtype.sh'
	local HW_TYPE=$( $cmd_hwtype)
	service_opt="\
        apg-dhcpd.service"
	[ $HW_TYPE == "VM" ] && {
		DHCP_STATE=$(${IMMLIST} -a value axeInfoId=apg_dhcp | cut -d "=" -f2)
		[ $DHCP_STATE == "OFF" ] && service_opt=""
		}
	# List of basic services to be monitored on the ACTIVE AP1 node
	local services_active_ap1="\
		apg-rsh.socket \
		apg-vsftpd.socket \
		apg-vsftpd-APIO_1.socket \
		apg-vsftpd-APIO_2.socket \
		apg-vsftpd-nbi.socket \
		apg-atftpd@192.168.169.$node_id.service \
		apg-atftpd@192.168.170.$node_id.service \
		apg-atftpd@192.168.169.33.service \
		apg-atftpd@192.168.170.33.service \
		rsyslog.service \
		lde-failoverd.service \
		auditd.service $service_opt"
	
	# List of basic services to be monitored on the PASSIVE AP1 node
	local services_passive_ap1="\
		apg-rsh.socket \
		rsyslog.service \
		lde-failoverd.service \
		auditd.service"

	# List of basic services to be monitored on the ACTIVE AP2 node
	local services_active_ap2="\
		apg-rsh.socket \
		apg-vsftpd.socket \
		apg-vsftpd-nbi.socket \
		rsyslog.service \
		lde-failoverd.service \
		auditd.service"
	
	# List of basic services to be monitored on the PASSIVE AP2 node
	local services_passive_ap2="\
		apg-rsh.socket \
		rsyslog.service \
		lde-failoverd.service \
		auditd.service"

	launchCommand0="$PS aux | $GREP com"
	launchCommand1="$CMW_STATUS node su comp si"
	launchCommand2="$APGADM -g locked"
		
	local AP=$(getAPType)
	if [ $AP -eq 1 ]; then 
		services_active=$services_active_ap1
		services_passive=$services_passive_ap1
	else
		services_active=$services_active_ap2
		services_passive=$services_passive_ap2
	fi
						
	# Checking health on local node
	# Check the status of service on active node
	for hc_service in $services_active
	do
		# Check the service status: the script returns:
		# - 0: if the service is running
		# - 1: if the service is not running
		${SERVICEMGMT} status $hc_service &> /dev/null 
		if [ $? -eq 1 ]; then
			flog "Service $hc_service not running on local node"
			return $exit_failure
		fi 
	done
	flog "All services up and running on local node"
	
	tmp_file=$(mktemp -t tempfile.XXXXXX)
 
        eval $launchCommand0 > $tmp_file
	result=$($CAT $tmp_file | $GREP -c /opt/com/bin/com)
	if [ $result -lt 1 ]; then
		flog "COM service not running on local node"
		rm $tmp_file
		return $exit_failure
	fi
		flog "COM service check succeeded on local node"

        eval $launchCommand1 > $tmp_file
	result=$?
	output=0
	checkHealth "local" $FUNCNAME
	if [ $? == $exit_failure ]; then
		flog "Check for opensafd failed on local node"
		rm $tmp_file
		return $exit_failure
	fi
		flog "Opensafd status succeeded on local node"	

	eval $launchCommand2 > $tmp_file
        if [ -s "$tmp_file" ]; then
        	flog "One or more APG service locked using apg-adm"
		rm $tmp_file
                return $exit_failure
        else
        	flog "apg-adm checks are succeeded on local node"
                flog "Healthcheck succeeded on local node"
        fi 
	
	# Check if -l option is specified
	if [ $a_local == $FALSE ]; then # If no -l option is specified, then check health of remote node
		# Checking status of peer node
		check_peer_node_state
		if [ $? == 0 ]; then
			# Launch command on remote node
			# Check the status of service on passive node
			for hc_service in $services_passive
			do
				# Check the service status: the script returns:
				# - 0: if the service is running
				# - 1: if the service is not running
				${SSH} -n $remote_node ${SERVICEMGMT} status $hc_service &> /dev/null
				if [ $? -eq 1 ]; then
					flog "Service $hc_service not running on remote node"
					rm $tmp_file					
					return $exit_failure
				fi 
			done
				flog "All services up and running on remote node"

                        ${SSH} -n $remote_node $launchCommand0 2>/dev/null > $tmp_file
			result=$($CAT $tmp_file | $GREP -c /opt/com/bin/com)
			if [ $result -lt 1 ]; then
				flog "COM service not running on remote node"
				rm $tmp_file
				return $exit_failure
			fi
				flog "COM service check succeeded on remote node"

			${SSH} -n $remote_node $launchCommand1 2>/dev/null > $tmp_file
			result=$?
			output=0
			checkHealth "remote" $FUNCNAME
			if [ $? == $exit_failure ]; then
				flog "Check for opensafd failed on remote node"
				rm $tmp_file
				return $exit_failure
			fi
				flog "Opensafd status succeeded on remote node"
	
			${SSH} -n $remote_node $launchCommand2 2>/dev/null > $tmp_file
                        if [ -s "$tmp_file" ]; then
                		flog "One or more APG service locked using apg-adm"
				rm $tmp_file
                                return $exit_failure
        		else
                		flog "apg-adm checks are succeeded on remote node"
				flog "Healthcheck succeeded on remote node"
        		fi

		else
			flog "Remote node is not up. Check to verify basic services failed on remote node"
			rm $tmp_file
			return $exit_failure
		fi
	fi
	rm $tmp_file

	trace_leave $FUNCNAME
	return $exit_success
}

function IO_HCRS_45(){

        trace_enter $FUNCNAME
        HW_TYPE=$(GetHwType)
        apt_type=$(cat /storage/system/config/apos/apt_type)
        if [[ $HW_TYPE != 'VM' ]] ;then
            flog "NTP monitoring is not supported for GEP"
            trace_leave $FUNCNAME
            return $exit_success
        fi

        launchCommand="ntpq -p | $GREP -Ev \"SC-2\" | $GREP \"INIT\""

        if [[ $apt_type == 'BSC' ]]; then
            launchCommand="chronyc sources | grep -Ev "SC-2" | grep -F '^?'"
        fi

        # Checking NTP health on local node
        result=$(eval $launchCommand)
        if [ $? == 0 ]; then
                flog "NTP health check failed on local node"
                return $exit_ntp_local
        fi
        flog "NTP health check success on local node"
        flog "Healthcheck succeeded on local node"


        # Checking status of peer node
        check_peer_node_state
        if [ $? == 0 ]; then
        #Launch command on remote node
                ${SSH} -n $remote_node $launchCommand 2>/dev/null
                if [ $? == 0 ]; then
                        flog "NTP health check failed on remote node"
                        return $exit_ntp_peer
                fi
                flog "NTP health check success on local node"
                flog "Healthcheck succeeded on remote node"
        else
             flog "Remote node is not up. Health not checked on remote node"
        fi

        trace_leave $FUNCNAME
        return $exit_success
}

#--------------------------------------------------------------------------------
# xsigano - 07-31-2020 HCSTART CHECK - CONNECTION BETWEEN ACTIVE AP AND CP
# This check works on AP1 active node
#---------------------------------------------------------------------------------
function IO_HCRS_46(){
        trace_enter $FUNCNAME
        # For mml commands to work
        APAMP="APAMP"

        if [ $a_local == $TRUE ]; then # If '-l' option is specified, rule should not be executed.
              trace_leave $FUNCNAME
              return $exit_success
        else 
              APAMP="APAMP"
              check_cp_state
              if [ $? -ne 0 ]; then
                 flog "Failed to retrieve CP information"
                 rm $tmp_file
                 return $exit_failure
              fi
              if [ $singleCP == 1 ]; then
                 launchCommand="$MML $APAMP"
              elif [ $multiCP == 1 ]; then
                 launchCommand="$MML -cp cp1 $APAMP"
              else
                 flog "Failed to retrieve CP information"
                 rm $tmp_file
                 return $exit_failure
              fi
              output1="ACTIVE"
              output2="PASSIVE"
              num2=2
              tmp_file=`mktemp -t tempfile.XXXXXX`
              getColumn="$CAT $tmp_file | $GREP -i status | $SED 's/ \+ /\n/g' | $GREP -ni status | $CUT -d: -f1"
              column_no=`eval $getColumn`
              node_id=$(</etc/cluster/nodes/this/id)
              if [ $node_id -gt 2 ]; then
                  flog "active node id is greater than 2"
                  rm $tmp_file
                  return $exit_failure
              fi
              parsePrintout="cat $tmp_file | $AWK -v v=$column_no '{print \$v}' | $SED '/^$/d'"
             
              # Checking health on local node
              eval $launchCommand 2>/dev/null > $tmp_file
              if [ $? -ne 0 ]; then
                  flog "Command launch failed on local node"
                  rm $tmp_file
                  return $exit_failure
              fi
              column_no1=`eval $getColumn`
             
              start_row=$(($node_id*$num2))
              flog "start row in APAMP $start_row"
              result=`cat $tmp_file | $AWK -v v=$column_no1 '{print \$v}' | $SED '/^$/d' | $TAIL -n +$start_row | $HEAD -2`
              for i in $(echo $result)
              do
                 if [[ $i == $output1 || $i == $output2 ]]; then
                      flog "AP maintenance data has $i state"
                 else
                      flog "AP maintenance data has $i state"
                      flog "Check for normal state of AP maintenance data failed on local node"
                      rm $tmp_file
                      return $exit_failure
                 fi
              done
              rm $tmp_file
              flog "Healthcheck succeeded"
             
              trace_leave $FUNCNAME
              return $exit_success
        fi
}
#-----------------------------------------------------------------------------
# xsigano - 07-31-2020 HCSTART CHECK - CHECK IF ANY SCBRP/SCX IN UNINIT
# This check works on AP1 active node
#-----------------------------------------------------------------------------
function IO_HCRS_47(){

        trace_enter $FUNCNAME

        check_cp_state
        if [ $? -ne 0 ]; then
           flog "Failed to retrieve CP information"
           return $exit_failure
        fi

        if [ $singleCP == 1 ]; then
          flog "Single CP node"  #SINGLE CP node just continue further
        elif [ $multiCP == 1 ]; then
           flog "This rule is not applicable on MultiCP node"
           trace_leave $FUNCNAME
           return $exit_success
        else
           flog "Failed to retrieve CP information"
           return $exit_failure
        fi

        application_info=`check_application_info`
        flog "application info $application_info"
        if [ $application_info -ne 2 ]; then # non BSC node
           flog "This rule is not applicable on non BSC node"
           trace_leave $FUNCNAME
           return $exit_success
        else
           flog "BSC node"
        fi

        tmp_file=`mktemp -t tempfile.XXXXXX`

        launchCommand1="$HWMSCBLS"
        launchCommand2="$HWMXLS"
        output1="master"
        output2="passive"
        getColumn="$CAT $tmp_file | $GREP -i status | $SED 's/ \+ /\n/g' | $GREP -ni status | $CUT -d: -f1"
        column_no=`eval $getColumn`

        shelf_info=`check_shelf_info`
        [ $shelf_info == 0 ] && launchCommand=$launchCommand1
        [ $shelf_info == 1 ] && launchCommand=$launchCommand2
       
        if [ $shelf_info == 2 ]; then # BSP architecture
           flog "This rule is not applicable on BSP architecture"
           trace_leave $FUNCNAME
           return $exit_success
        fi

        if [ $shelf_info == 4 ]; then # SMX architecture
           flog "Ignore rule on SMX architecture"
           trace_leave $FUNCNAME
           return $exit_success
        fi

        if [ $shelf_info -gt 4 ]; then # Shelf architecure is not valid
           flog "Invalid Shelf architecture"
           trace_leave $FUNCNAME
           return $exit_failure
        fi

        # Checking health on local node
        for i in 1 2 3 4 5 6
        do           
           $launchCommand > $tmp_file
           if [ $? -ne 0 ]; then
               flog "Command launch failed on local node"
               rm $tmp_file
               return $exit_failure
           fi

           column_no1=`eval $getColumn`
           result=`cat $tmp_file | $AWK -v v=$column_no1 '{print \$v}' | $SED '/^$/d' | $TAIL -n +2`
           
           for i in $(echo $result)
           do
                if [[ $i == $output1 || $i == $output2 ]]; then
                        flog "SWITCH BOARD has $i state"
                        #echo "SWITCH BOARD has $i state"
                else
                        flog "SWITCH BOARD has $i state"
                        #echo "SWITCH BOARD has $i state"
                        flog "Check for normal state of SWITCH BOARD failed on local node"
                        rm $tmp_file
                        return $exit_failure
                fi
            done
            rm $tmp_file
            sleep 1
        done

        flog "Check for Master/Passive SCB-RP/SCXB succeeded on local node"
        flog "Healthcheck succeeded"

        trace_leave $FUNCNAME
        return $exit_success
}
#--------------------------------------------------------------------------
# xsigano - 07-31-2020 HCSTART CHECK - CHECK IF ANY SMXB IN UNINIT
# This check works on AP1 active node
#-------------------------------------------------------------------------
function IO_HCRS_48(){

        trace_enter $FUNCNAME

        check_cp_state
        if [ $? -ne 0 ]; then
           flog "Failed to retrieve CP information"
           return $exit_failure
        fi

        if [ $singleCP == 1 ]; then
          flog "Single CP node"  #SINGLE CP node just continue further
        elif [ $multiCP == 1 ]; then
           flog "This rule is not applicable on MultiCP node"
           trace_leave $FUNCNAME
           return $exit_success
        else
           flog "Failed to retrieve CP information"
           return $exit_failure
        fi

        application_info=`check_application_info`
        flog "application info $application_info"
        if [ $application_info -ne 2 ]; then # non BSC node
           flog "This rule is not applicable on non BSC node"
           trace_leave $FUNCNAME
           return $exit_success
        else
           flog "BSC node"
        fi


        tmp_file=`mktemp -t tempfile.XXXXXX`

        launchCommand1="$HWMXLS"
        output1="master"
        output2="passive"
        getColumn="$CAT $tmp_file | $GREP -i status | $SED 's/ \+ /\n/g' | $GREP -ni status | $CUT -d: -f1"
        column_no=`eval $getColumn`

        shelf_info=`check_shelf_info`
        
        [ $shelf_info == 4 ] && launchCommand=$launchCommand1

        if [ $shelf_info == 0 ] || [ $shelf_info == 1 ]; then
           flog "Ignore rule on SCX/SCBRP architecture" 
        fi

        if [ $shelf_info == 2 ]; then # BSP architecture
           flog "This rule is not applicable on BSP architecture"
           trace_leave $FUNCNAME
           return $exit_success
        fi

        if [ $shelf_info -gt 4 ]; then # Shelf architecure is not valid
           flog "Invalid Shelf architecture"
           trace_leave $FUNCNAME
           return $exit_failure
        fi
        # Checking health on local node
        for i in 1 2 3 4 5 6
        do           
           $launchCommand > $tmp_file
           if [ $? -ne 0 ]; then
               flog "Command launch failed on local node"
               rm $tmp_file
               return $exit_failure
           fi

           column_no1=`eval $getColumn`
           result=`cat $tmp_file | $AWK -v v=$column_no1 '{print \$v}' | $SED '/^$/d' | $TAIL -n +2`
           
           for i in $(echo $result)
           do
                if [[ $i == $output1 || $i == $output2 ]]; then
                        flog "SWITCH BOARD has $i state"
                        #echo "SWITCH BOARD has $i state"
                else
                        flog "SWITCH BOARD has $i state"
                        #echo "SWITCH BOARD has $i state"
                        flog "Check for normal state of SWITCH BOARD failed on local node"
                        rm $tmp_file
                        return $exit_failure
                fi
            done
            rm $tmp_file
            sleep 1
        done

        flog "Check for Master/Passive SMXB succeeded on local node"
        flog "Healthcheck succeeded"

        trace_leave $FUNCNAME
        return $exit_success
}
#--------------------------------------------------------------------------------------------
# xsigano - 07-31-2020 HCSTART IMPROVEMENTS - CHECK IF APTCMM upgrade/installation is ongoing
# This check works on AP1 active node
#---------------------------------------------------------------------------------------------
function IO_HCRS_49(){

        trace_enter $FUNCNAME

	check_cp_state
						
        if [ $? -ne 0 ]; then
           flog "Failed to retrieve CP information"
           return $exit_failure
        fi

	if [ $singleCP == 1 ]; then
          flog "Single CP node"  #SINGLE CP node just continue further
        elif [ $multiCP == 1 ]; then
           flog "This rule is not applicable on MultiCP node"
           trace_leave $FUNCNAME 
           return $exit_success        
        else
           flog "Failed to retrieve CP information"
           return $exit_failure
        fi
		
        application_info=`check_application_info`
        flog "application info $application_info"
        if [ $application_info -ne 2 ]; then # non BSC node
           flog "This rule is not applicable on non BSC node"
           trace_leave $FUNCNAME
           return $exit_success
        else
           flog "BSC node" 
        fi
		
	aptcmm_upgradestatus=`check_aptcmm_UpgradeStatus`
	
        if [ "$aptcmm_upgradestatus" != "DONE" ]; then 
           flog "APTCMM upgrade is ongoing"
           return $exit_failure
        fi	
		
	aptcmm_installationstatus=`check_aptcmm_InstallationStatus`
	if [ "$aptcmm_installationstatus" != "DONE" ]; then 
           flog "APTCMM installation is ongoing"
           return $exit_failure
        fi	
   
        flog "No APTCMM upgrade or installation is ongoing"
        trace_leave $FUNCNAME
        return $exit_success
}
#--------------------------------------------------------------------------
# ZPRAPXX: This function checks for the DRBD[0/1] role misalignment and faulty disk status
# 
#-------------------------------------------------------------------------
function IO_HCRS_50(){
        local node_state
        local active_nodeid
        local passive_nodeid
        local disk_states=( 'Attaching' 'Negotiating' 'Outdated' 'Consistent' 'UpToDate' 'Inconsistent' ) 
        trace_enter $FUNCNAME
        #For fetching the active and passive node id's 
        for node_id in 1 2
        do
          if [ -f $is_swm_2_0 ];then
            node_state=$(${IMMLIST} -a saAmfSISUHAState "safSISU=safSu=SC-$node_id\,safSg=2N\,safApp=ERIC-apg.nbi.aggregation.service,safSi=apg.nbi.aggregation.service-2N-1,safApp=ERIC-apg.nbi.aggregation.service" 2> /dev/null | ${CUT} -d = -f2 )
          else
            node_state=$(${IMMLIST} -a saAmfSISUHAState "safSISU=safSu=$node_id\,safSg=2N\,safApp=ERIC-APG,safSi=AGENT,safApp=ERIC-APG" 2> /dev/null | ${CUT} -d = -f2 )
          fi
          if [ "$node_state" == "1" ]; then 
            active_nodeid=$node_id
          elif [ "$node_state" == "2" ]; then
            passive_nodeid=$node_id
	  else
            active_nodeid=$(</etc/cluster/nodes/this/id)
            passive_nodeid=$(</etc/cluster/nodes/peer/id)
          fi
        done 

        #The loop repeats twice,once for drbd0 and drbd1(if present)
        for i in 0 1
        do
          local disk_flag=$FALSE 
          launchcommand1="$DRBDADM status drbd$i  2>/dev/null| grep -o \"drbd$i.*\" | ${CUT} -d : -f2 | $AWK '{print \$1}'"
          active_drbd_role=$(eval $launchcommand1)
          if [ $? -ne 0 ]; then 
          flog "Error executing drbdadm status drbd$i command"
          return $exit_failure 
          fi
          if [ "$active_drbd_role" != "Primary" ]; then 
            flog "ERROR: SC-2-$active_nodeid drbd$i role is faulty,\"drbdadm status\" command printout:"
            flog "$(eval $DRBDADM status)"
            flog "\"drbd-overview\" command printout:"
            flog "$(eval drbd-overview)"
            return $exit_failure
          fi    
          flog "SC-2-$active_nodeid drbd$i role: $active_drbd_role"

          launchcommand2="$DRBDADM status drbd$i 2>/dev/null | grep -o \" disk.*\" | ${CUT} -d : -f2 | $AWK '{print \$1}'"
          active_disk_state=$(eval $launchcommand2)
          if  [ $? -ne 0 ];then  
          flog "Error executing drbdadm status drbd$i command"
          return $exit_failure
          fi 
          for state in ${disk_states[@]}; do
            if [ "$active_disk_state" == "$state" ]; then 
                disk_flag=$TRUE
                break;
            fi    
          done 
          if [ $disk_flag -ne $TRUE ];then 
            flog "ERROR: SC-2-$active_nodeid disk status is faulty,\"drbdadm status\" command printout:"
            flog "$(eval $DRBDADM status)"
            flog "\"drbd-overview\" command printout:"
            flog "$(eval drbd-overview)"
            return  $exit_failure 
          fi
          flog "SC-2-$active_nodeid drbd$i disk state: $active_disk_state"

          disk_flag=$FALSE
          launchcommand3="$DRBDADM status drbd$i 2>/dev/null | grep -o \"SC-2-$passive_nodeid.*\" | ${CUT} -d : -f2 | $AWK '{print \$1}'"
          passive_drbd_role=$(eval $launchcommand3)
          if [ $? -ne 0 ]; then
          flog "Error executing drbdadm status drbd$i command"
          return $exit_failure 
          fi
          if [ "$passive_drbd_role" != "Secondary" ]; then
            flog "ERROR: SC-2-$passive_nodeid drbd$i role is faulty,\"drbdadm status\" command printout:"
            flog "$(eval $DRBDADM status)"
            flog "\"drbd-overview\" command printout:"
            flog "$(eval drbd-overview)"
            return $exit_failure
          fi
          flog "SC-2-$passive_nodeid drbd$i role: $passive_drbd_role"
          
          launchcommand4="$DRBDADM status drbd$i 2>/dev/null | grep -o \"peer-disk.*\" | ${CUT} -d : -f2 | $AWK '{print \$1}'"
          passive_disk_state=$(eval $launchcommand4)
          if [ $? -ne 0 ] ; then
          flog "Error executing drbdadm status drbd$i command"
          return $exit_failure 
          fi
          for state in ${disk_states[@]}; do
            if [ "$passive_disk_state" == "$state" ]; then
                disk_flag=$TRUE
                break;
            fi
          done
          if [ $disk_flag -ne $TRUE ]; then
            flog "ERROR: SC-2-$passive_nodeid drbd$i disk state  is faulty, \"drbdadm status\" command printout:"
            flog "$(eval $DRBDADM status)"
            flog "\"drbd-overview\" command printout:"
            flog "$(eval drbd-overview)"
            return $exit_failure 
          fi
          flog "SC-2-$passive_nodeid drbd$i disk state: $passive_disk_state"
          if [ "$(GetHwType)" == "GEP2" ]; then 
            break;
          fi
        done
      
        flog "DRBD[0/1] configurations are in proper state"
        trace_leave $FUNCNAME
        return $exit_success
}
#--------------------------------------------------------------------------------------------
# xpamsur - 12-08-2020 HCSTART IMPROVEMENTS - CHECK if APG File system was changed to EXT4
#---------------------------------------------------------------------------------------------
function IO_HCRS_51(){

        trace_enter $FUNCNAME

        local launchCommand1="timeout 20 lsblk -f|grep -i ext4|cut -d "-" -f2-"

        local impactedDueToExt4FileSystem=`eval $launchCommand1`
        if [ ! -z "$impactedDueToExt4FileSystem" ]; then
                flog "In Local node, change in File System (EXT4) detected for "
                flog "$impactedDueToExt4FileSystem"
                return $exit_failure
        else
                flog "APG File System is Ext3"
        fi

        flog "APG file system check succeeded on local node"

        flog "Healthcheck succeeded on local node"

        # Check if -l option is specified
        if [ $a_local == $FALSE ]; then # If no -l option is specified, then check health of remote node
                # Checking status of peer node
                check_peer_node_state
                if [ $? == 0 ]; then
                        # Launch command on remote node
                        impactedDueToExt4FileSystem=$(${SSH} -n $remote_node $launchCommand1 2>/dev/null)
                        if [ ! -z "$impactedDueToExt4FileSystem" ]; then
                                flog "In Remote node, change in File System (EXT4) detected for "
                                flog "$impactedDueToExt4FileSystem"
                                return $exit_failure
                        else
                                flog "APG File System is Ext3"
                        fi

                flog "APG file system check succeeded on remote node"

                flog "Healthcheck succeeded on remote node"
                else
                        flog "Remote node is not up. Health not checked on remote node"
                        #return $exit_failure #Commenting because If the remote node is not reachable then the rule will fail.And a slogan as FS change will be reported.Which is not completely true 
                fi
        fi

        trace_leave $FUNCNAME
        return $exit_success

}

#--------------------------------------------------------------------------------
# xcsrpad - 03-12-2020 HCSTART CHECK - CHECK FOR ALARMS "CPT FAULT" or "CP FAULT"
# or "CP AP COMMUNICATION FAULT" or "CP AP COMMUNICATION FAULT "
# This check works on AP1 active node
#---------------------------------------------------------------------------------
function IO_HCRS_52(){
alarm=""
ALARM1="CPT FAULT"
ALARM2="CP FAULT"
ALARM3="AP NOT AVAILABLE"
ALARM4="CP AP COMMUNICATION FAULT"
        trace_enter $FUNCNAME
        # For mml commands to work
        ALLIP="ALLIP"

        if [ $a_local == $TRUE ]; then # If '-l' option is specified, rule should not be executed.
              return $exit_success
        else
              node_id=$(</etc/cluster/nodes/this/id)
              if [ $node_id -gt 2 ]; then
                  flog "active node id is greater than 2"
                  return $exit_failure
              fi
              ALLIP="ALLIP"
              check_cp_state
              if [ $? -ne 0 ]; then
                 flog "Failed to retrieve CP information"
                 return $exit_success
              fi

              if [ $singleCP == 1 ]; then
                 launchCommand="$MML $ALLIP"
                 tmp_file=`mktemp -t tempfile.XXXXXX`
                 eval $launchCommand  2>/dev/null > $tmp_file
                 if [ $? -ne 0 ]; then
                    flog "Command launch failed "
                    rm $tmp_file
                    return $exit_success
                 fi
                 alarm=`cat $tmp_file | $GREP -e "$ALARM1" -e "$ALARM2" -e "$ALARM3" -e "$ALARM4" `
                 if [ $? -eq 0 ]; then
                    alarm=`echo $alarm|head -1`
                    flog "$alarm is present on the CP"
                    rm $tmp_file
                    return $exit_failure
                 fi
                 rm $tmp_file
                 elif [ $multiCP == 1 ]; then
                     launchCommand="$MML -cp "
                     #fetch the number of dual and single blades
                     CP=`immfind|grep -i dualSidedCpId=CP|grep -v "swM" |awk -F, {'print $1'}|awk -F= {'print $2'}`
                     BC=`immfind |grep -i ClusterCp|awk -F, {'print $1'}|awk -F= {'print $2'}`
                     for i in $(echo $CP )
                     do
                         tmp_file=`mktemp -t tempfile.XXXXXX`
                         eval $launchCommand $i $ALLIP 2>/dev/null > $tmp_file
                         if [ $? -ne 0 ]; then
                            flog "Command launch failed on $i"
                            rm $tmp_file
                            continue
                         fi
                         alarm=`cat $tmp_file | $GREP -e "$ALARM1" -e "$ALARM2" -e "$ALARM3" `
                         if [ $? -eq 0 ]; then
                            alarm=`echo $alarm|head -1`
                            flog "$alarm is present on $i"
                            rm $tmp_file
                            return $exit_failure
                         fi
                         rm $tmp_file
                    done
                    for j in $(echo $BC )
                    do
                       tmp_file=`mktemp -t tempfile.XXXXXX`
                       #launchCommand=echo $launchCommand + " " echo $j echo " " echo $ALLIP
                       eval $launchCommand $j $ALLIP 2>/dev/null > $tmp_file
                       if [ $? -ne 0 ]; then
                          flog "Command launch failed on $j"
                          rm $tmp_file
                          continue
                       fi
                       alarm=`cat $tmp_file | $GREP -e "$ALARM1" -e "$ALARM2" -e "$ALARM3" `
                       if [ $? -eq 0 ]; then
                          alarm=`echo $alarm|head -1`
                          flog "$alarm is present on $j"
                          rm $tmp_file
                          return $exit_failure
                       fi
                       rm $tmp_file
                  done
             else
                 flog "Failed to retrieve CP information"
                 return $exit_success
             fi

             flog "Healthcheck succeeded no major CP alarms found "
             trace_leave $FUNCNAME
             return $exit_success
        fi
}

#--------------------------------------------------------------------------------
# zprapxx/xcsrpad - 07-12-2020 HCSTART CHECK - Check for File System Corruption "
#---------------------------------------------------------------------------------
function IO_HCRS_53(){
            trace_enter $FUNCNAME
            local launchcommand
            local corrupt_file            
            local cflag=$FALSE
 
            for path in data cluster
            do
              corrupt_file=""
              launchcommand="ls -lR -I nbi_fuse /$path 2> /dev/null | grep \"\?\" | awk '{print \$NF}' |tr '\n' ',' | sed 's/.$//'"
              corrupt_file=$(eval $launchcommand)
              if [ "$corrupt_file" != "" ]; then
                flog "File system corruption in /$path folder: corrupted file(s): $corrupt_file "
                cflag=$TRUE
              else
                flog "No File system corruption in /$path folder"
              fi
              if [[ $a_local == $FALSE && "$path" == "data" ]]; then  # If no -l option is specified, then check health of remote node
                # Checking status of peer node
                check_peer_node_state
                if [ $? == 0 ]; then
                  # Launch command on remote node
                  corrupt_file=$(${SSH} -n $remote_node $launchcommand 2>/dev/null)      
                  if [ "$corrupt_file" != "" ]; then
                    flog "File system corruption in /$path folder on peer($remote_node) : corrupted file(s): $corrupt_file "
                    cflag=$TRUE
                  else
                    flog "No File system corruption in /$path folder on peer($remote_node)"
                  fi
                else
                  flog "Remote node not reachable, file system check incomplete"
                 # return $exit_failure
                fi
              fi 
            done
        
            if [ $cflag -eq $TRUE ]; then
                 return $exit_failure
            fi
            
            trace_leave $FUNCNAME
            return $exit_success
}

#------------------------------------------------------------------------------------
# ZPRAPXX: This function checks if the LDAP primary/secondary servers are reachable or not
# 
#------------------------------------------------------------------------------------
function IO_HCRS_54(){
        local cacert=""
        local cert
        local key
        local pri_ldap_addr
        local sec_ldap_addr
        local ldap_server_port 
        local pri_ldap_res=$FALSE
        local sec_ldap_res=$FALSE 
        local CACERT_ARRAY=""
	local node_cred
        tmp_file=`$MKTEMP -t tempfile.XXXXXX`
        trace_enter $FUNCNAME

        pri_ldap_addr=$(immlist -a ldapIpAddress ldapId=1,SecLdapAuthenticationldapAuthenticationMethodId=1 | ${CUT} -d = -f2 )
        if [ "$pri_ldap_addr" == "<Empty>" ]; then
          flog "LDAP is not configured, exiting the rule"
          rm $tmp_file
          return $exit_success
        fi

        sec_ldap_addr=$(immlist -a fallbackLdapIpAddress ldapId=1,SecLdapAuthenticationldapAuthenticationMethodId=1 | ${CUT} -d = -f2 )
        if [ "$sec_ldap_addr" == "<Empty>" ]; then
          flog "Secondary LDAP server is not configured"
        fi
        
        ldap_server_port=$(immlist -a serverPort ldapId=1,SecLdapAuthenticationldapAuthenticationMethodId=1 | ${CUT} -d = -f2 )
        if [ "$ldap_server_port" == "<Empty>" ]; then
          use_tls=$(immlist -a useTls ldapId=1,SecLdapAuthenticationldapAuthenticationMethodId=1 | ${CUT} -d = -f2 )
	  if [ "$use_tls" == "1" ]; then
            tls_mode=$(immlist -a tlsMode ldapId=1,SecLdapAuthenticationldapAuthenticationMethodId=1 | ${CUT} -d = -f2 )
            if [ "$tls_mode" == "1" ]; then
              ldap_server_port=636
            else
              ldap_server_port=389
            fi
          else
	    ldap_server_port=389
	  fi
        flog "LDAP Server Port is $ldap_server_port"
        fi
      
        tmp_cacert=$(immlist -a tlsCaCertificate ldapConfigurationId=1,AxeEquipmentequipmentMId=1 | ${CUT} -d = -f2)
        if [ ! -z $tmp_cacert ]; then
          CACERT_ARRAY=$(find $tmp_cacert -name trusted_* 2>/dev/null)
        fi 
        cert=$(immlist -a tlsClientCertificate ldapConfigurationId=1,AxeEquipmentequipmentMId=1 | ${CUT} -d = -f2)
        key=$(immlist -a tlsClientKey ldapConfigurationId=1,AxeEquipmentequipmentMId=1 | ${CUT} -d = -f2)
        if [ -z "$CACERT_ARRAY" ];then
          flog "Unable to fetch the certificate"
          rm $tmp_file
          return $exit_failure
        fi
        
	node_cred=$(immlist -a nodeCredential ldapId=1,SecLdapAuthenticationldapAuthenticationMethodId=1 | ${CUT} -d = -f2)
	if [ "$node_cred" != "<Empty>" ]; then
        	for cacert in ${CACERT_ARRAY[@]}
        	do
          	$(timeout 6 openssl s_client -host $pri_ldap_addr -port $ldap_server_port -tls1 -cert $cert -key $key -CAfile $cacert -cipher RSA -status 2>/dev/null > $tmp_file)
          	verify_conn=$($CAT $tmp_file | grep "Verify return code: 0 (ok)")
          	if [ $? -eq 0 ];then
            	pri_ldap_res=$TRUE
          	fi
        	done
	else
		for cacert in ${CACERT_ARRAY[@]}
                do
                $(timeout 6 openssl s_client -host $pri_ldap_addr -port $ldap_server_port -tls1 -CAfile $cacert -cipher RSA -status 2>/dev/null > $tmp_file)
                verify_conn=$($CAT $tmp_file | grep "Verify return code: 0 (ok)")
                if [ $? -eq 0 ];then
                pri_ldap_res=$TRUE
                fi
                done
	fi
        
        if [ $pri_ldap_res -eq $FALSE ]; then
          flog "Openssl command execution failed, The Primary LDAP server is unreachable"
          rm $tmp_file
          return $exit_failure
        fi


        if [ "$sec_ldap_addr" != "<Empty>" ]; then
		if [ "$node_cred" != "<Empty>" ]; then
		
			for cacert in ${CACERT_ARRAY[@]}
        	 	do
            		$(timeout 6 openssl s_client -host $sec_ldap_addr -port $ldap_server_port -tls1 -cert $cert -key $key -CAfile $cacert -cipher RSA -status 2>/dev/null > $tmp_file)
            		verify_conn=$($CAT $tmp_file | grep "Verify return code: 0 (ok)")
            		if [ $? -eq 0 ];then
             	 	sec_ldap_res=$TRUE
            		fi
          		done
		else

	                for cacert in ${CACERT_ARRAY[@]}
        	        do
       		        $(timeout 6 openssl s_client -host $sec_ldap_addr -port $ldap_server_port -tls1 -CAfile $cacert -cipher RSA -status 2>/dev/null > $tmp_file)
              		verify_conn=$($CAT $tmp_file | grep "Verify return code: 0 (ok)")
                	if [ $? -eq 0 ];then
                	sec_ldap_res=$TRUE
                	fi
                	done
		fi

		if [ $sec_ldap_res -eq $FALSE ]; then
	                flog "Openssl command execution failed, The Secondary LDAP server is unreachable"
        	        rm $tmp_file
                	return $exit_failure
        	fi

	fi


        rm $tmp_file
        flog "The configured LDAP servers are reachable"
        trace_leave $FUNCNAME
        return $exit_success
 
}

#------------------------------------------------------------------------
# ZBHEGNA: This function checks for iomem=relaxed kernel cmdline parameter
#------------------------------------------------------------------------
function IO_HCRS_55(){
        trace_enter $FUNCNAME

        local iomem_param="iomem=relaxed"
        local cmdline_file="/proc/cmdline"

        #check for iomem parameter in current node
        $GREP -q $iomem_param $cmdline_file 2>/dev/null
        if [ $? -eq 0 ];then
          flog "iomem kernel parameter is present"
          return $exit_failure
        fi
        check_peer_node_state
        if [ $? -eq 0 ];then
          #check for iomem parameter in remote node
          ${SSH} -n $remote_node $GREP -q $iomem_param $cmdline_file 2>/dev/null
          if [ $? -eq 0 ];then
            flog "iomem kernel parameter is present in remote node"
            return $exit_failure
          fi
        fi

        trace_leave $FUNCNAME
        return $exit_success
}

#-----------------------------------------------------------------------------
# ZMEDRAM: This function checks for boot-admin user account is defined on APG
#-----------------------------------------------------------------------------
function IO_HCRS_56(){
        trace_enter $FUNCNAME
 
        local grubcfg_file="/boot/grub2/grub.cfg"
        local grub_para="password_pbkdf2 boot-admin"

        $GREP -q $grub_para $grubcfg_file 2>/dev/null
        if [ $? -eq 0 ];then
          flog "boot admin user exists on local node"
          flog "healthcheck succeeded on local node"
        else
          flog "boot admin user does not exists on local node"
          return $exit_failure
        fi
        check_peer_node_state
        if [ $? -eq 0 ];then
          ${SSH} -n $remote_node $GREP -q $grub_para $grubcfg_file 2>/dev/null
          if [ $? -eq 0 ];then
            flog "boot admin user exists on remote node"
            flog "healthcheck succeeded on remote node"
          else
            flog "boot admin user does not exists on remote node"
            return $exit_failure
          fi
        fi

        trace_leave $FUNCNAME
        return $exit_success
}

